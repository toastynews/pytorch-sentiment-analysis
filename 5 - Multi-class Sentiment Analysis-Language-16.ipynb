{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hong Kongese Language Identifier\n",
    "This notebook contains modifications to make it run with the Hong Kongese language identification dataset. The only difference is that we do not load the English vectors because they will be useless on Hong Kongese.\n",
    "This notebook uses a dataset with 16 each of Hong Kongese and Standard Chinese articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Multi-class Sentiment Analysis\n",
    "\n",
    "In all of the previous notebooks we have performed sentiment analysis on a dataset with only two classes, positive or negative. When we have only two classes our output can be a single scalar, bound between 0 and 1, that indicates what class an example belongs to. When we have more than 2 examples, our output must be a $C$ dimensional vector, where $C$ is the number of classes.\n",
    "\n",
    "In this notebook, we'll be performing classification on a dataset with 6 classes. Note that this dataset isn't actually a sentiment analysis dataset, it's a dataset of questions and the task is to classify what category the question belongs to. However, everything covered in this notebook applies to any dataset with examples that contain an input sequence belonging to one of $C$ classes.\n",
    "\n",
    "Below, we setup the fields, and load the dataset. \n",
    "\n",
    "The first difference is that we do not need to set the `dtype` in the `LABEL` field. When doing a mutli-class problem, PyTorch expects the labels to be numericalized `LongTensor`s. \n",
    "\n",
    "The second different is that we use `TREC` instead of `IMDB` to load the `TREC` dataset. The `fine_grained` argument allows us to use the fine-grained labels (of which there are 50 classes) or not (in which case they'll be 6 classes). You can change this how you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATASET=\"16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tokenizer to simply split at character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    return list(map(str, text.replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from data/language directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenizer)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "fields = {'language': ('label', LABEL), 'text': ('text', TEXT)}\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = 'data/language/' + DATASET,\n",
    "                                        train = 'train.json',\n",
    "                                        validation = 'valid.json',\n",
    "                                        test = 'test.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'zh',\n",
       " 'text': ['港',\n",
       "  '區',\n",
       "  '人',\n",
       "  '大',\n",
       "  '代',\n",
       "  '表',\n",
       "  '年',\n",
       "  '底',\n",
       "  '換',\n",
       "  '屆',\n",
       "  '，',\n",
       "  '港',\n",
       "  '區',\n",
       "  '人',\n",
       "  '大',\n",
       "  '常',\n",
       "  '委',\n",
       "  '范',\n",
       "  '徐',\n",
       "  '麗',\n",
       "  '泰',\n",
       "  '接',\n",
       "  '受',\n",
       "  '傳',\n",
       "  '媒',\n",
       "  '訪',\n",
       "  '問',\n",
       "  '時',\n",
       "  '，',\n",
       "  '表',\n",
       "  '明',\n",
       "  '不',\n",
       "  '再',\n",
       "  '尋',\n",
       "  '求',\n",
       "  '留',\n",
       "  '任',\n",
       "  '，',\n",
       "  '也',\n",
       "  '同',\n",
       "  '時',\n",
       "  '批',\n",
       "  '評',\n",
       "  '香',\n",
       "  '港',\n",
       "  '很',\n",
       "  '多',\n",
       "  '新',\n",
       "  '一',\n",
       "  '代',\n",
       "  '的',\n",
       "  '從',\n",
       "  '政',\n",
       "  '者',\n",
       "  '「',\n",
       "  '恃',\n",
       "  '票',\n",
       "  '傲',\n",
       "  '物',\n",
       "  '」',\n",
       "  '，',\n",
       "  '拒',\n",
       "  '絕',\n",
       "  '理',\n",
       "  '解',\n",
       "  '及',\n",
       "  '溝',\n",
       "  '通',\n",
       "  '。',\n",
       "  '她',\n",
       "  '認',\n",
       "  '為',\n",
       "  '這',\n",
       "  '是',\n",
       "  '導',\n",
       "  '致',\n",
       "  '香',\n",
       "  '港',\n",
       "  '社',\n",
       "  '會',\n",
       "  '撕',\n",
       "  '裂',\n",
       "  '及',\n",
       "  '議',\n",
       "  '會',\n",
       "  '對',\n",
       "  '立',\n",
       "  '的',\n",
       "  '主',\n",
       "  '要',\n",
       "  '原',\n",
       "  '因',\n",
       "  '。',\n",
       "  '范',\n",
       "  '太',\n",
       "  '可',\n",
       "  '以',\n",
       "  '經',\n",
       "  '常',\n",
       "  '高',\n",
       "  '調',\n",
       "  '指',\n",
       "  '點',\n",
       "  '江',\n",
       "  '山',\n",
       "  '，',\n",
       "  '教',\n",
       "  '育',\n",
       "  '香',\n",
       "  '港',\n",
       "  '人',\n",
       "  '應',\n",
       "  '該',\n",
       "  '如',\n",
       "  '何',\n",
       "  '愛',\n",
       "  '國',\n",
       "  '，',\n",
       "  '原',\n",
       "  '因',\n",
       "  '當',\n",
       "  '然',\n",
       "  '是',\n",
       "  '因',\n",
       "  '為',\n",
       "  '她',\n",
       "  '本',\n",
       "  '身',\n",
       "  '也',\n",
       "  '可',\n",
       "  '以',\n",
       "  '說',\n",
       "  '是',\n",
       "  '政',\n",
       "  '壇',\n",
       "  '的',\n",
       "  '元',\n",
       "  '老',\n",
       "  '級',\n",
       "  '人',\n",
       "  '物',\n",
       "  '，',\n",
       "  '曾',\n",
       "  '經',\n",
       "  '擔',\n",
       "  '任',\n",
       "  '主',\n",
       "  '權',\n",
       "  '移',\n",
       "  '交',\n",
       "  '後',\n",
       "  '的',\n",
       "  '立',\n",
       "  '法',\n",
       "  '會',\n",
       "  '主',\n",
       "  '席',\n",
       "  '，',\n",
       "  '擔',\n",
       "  '任',\n",
       "  '人',\n",
       "  '大',\n",
       "  '常',\n",
       "  '委',\n",
       "  '也',\n",
       "  '已',\n",
       "  '是',\n",
       "  '經',\n",
       "  '年',\n",
       "  '。',\n",
       "  '不',\n",
       "  '過',\n",
       "  '，',\n",
       "  '不',\n",
       "  '久',\n",
       "  '之',\n",
       "  '前',\n",
       "  '她',\n",
       "  '還',\n",
       "  '公',\n",
       "  '開',\n",
       "  '說',\n",
       "  '自',\n",
       "  '己',\n",
       "  '也',\n",
       "  '說',\n",
       "  '不',\n",
       "  '清',\n",
       "  '楚',\n",
       "  '多',\n",
       "  '年',\n",
       "  '來',\n",
       "  '在',\n",
       "  '這',\n",
       "  '個',\n",
       "  '崗',\n",
       "  '位',\n",
       "  '為',\n",
       "  '港',\n",
       "  '人',\n",
       "  '爭',\n",
       "  '取',\n",
       "  '過',\n",
       "  '甚',\n",
       "  '麼',\n",
       "  '。',\n",
       "  '按',\n",
       "  '道',\n",
       "  '理',\n",
       "  '，',\n",
       "  '她',\n",
       "  '應',\n",
       "  '該',\n",
       "  '是',\n",
       "  '代',\n",
       "  '表',\n",
       "  '香',\n",
       "  '港',\n",
       "  '人',\n",
       "  '發',\n",
       "  '言',\n",
       "  '，',\n",
       "  '但',\n",
       "  '似',\n",
       "  '乎',\n",
       "  '擔',\n",
       "  '當',\n",
       "  '這',\n",
       "  '公',\n",
       "  '職',\n",
       "  '這',\n",
       "  '麼',\n",
       "  '多',\n",
       "  '年',\n",
       "  '，',\n",
       "  '從',\n",
       "  '來',\n",
       "  '不',\n",
       "  '需',\n",
       "  '要',\n",
       "  '向',\n",
       "  '市',\n",
       "  '民',\n",
       "  '交',\n",
       "  '代',\n",
       "  '。',\n",
       "  '她',\n",
       "  '能',\n",
       "  '夠',\n",
       "  '長',\n",
       "  '期',\n",
       "  '盤',\n",
       "  '踞',\n",
       "  '這',\n",
       "  '個',\n",
       "  '崗',\n",
       "  '位',\n",
       "  '，',\n",
       "  '也',\n",
       "  '不',\n",
       "  '見',\n",
       "  '得',\n",
       "  '是',\n",
       "  '得',\n",
       "  '到',\n",
       "  '由',\n",
       "  '香',\n",
       "  '港',\n",
       "  '市',\n",
       "  '民',\n",
       "  '授',\n",
       "  '權',\n",
       "  '，',\n",
       "  '純',\n",
       "  '粹',\n",
       "  '只',\n",
       "  '是',\n",
       "  '一',\n",
       "  '個',\n",
       "  '由',\n",
       "  '長',\n",
       "  '官',\n",
       "  '意',\n",
       "  '志',\n",
       "  '控',\n",
       "  '制',\n",
       "  '下',\n",
       "  '的',\n",
       "  '體',\n",
       "  '制',\n",
       "  '，',\n",
       "  '透',\n",
       "  '過',\n",
       "  '小',\n",
       "  '圈',\n",
       "  '子',\n",
       "  '選',\n",
       "  '舉',\n",
       "  '產',\n",
       "  '生',\n",
       "  '。',\n",
       "  '爭',\n",
       "  '取',\n",
       "  '不',\n",
       "  '到',\n",
       "  '甚',\n",
       "  '麼',\n",
       "  '，',\n",
       "  '或',\n",
       "  '沒',\n",
       "  '有',\n",
       "  '為',\n",
       "  '香',\n",
       "  '港',\n",
       "  '人',\n",
       "  '爭',\n",
       "  '取',\n",
       "  '甚',\n",
       "  '麼',\n",
       "  '，',\n",
       "  '也',\n",
       "  '絕',\n",
       "  '不',\n",
       "  '令',\n",
       "  '人',\n",
       "  '感',\n",
       "  '到',\n",
       "  '意',\n",
       "  '外',\n",
       "  '了',\n",
       "  '。',\n",
       "  '過',\n",
       "  '去',\n",
       "  '多',\n",
       "  '年',\n",
       "  '來',\n",
       "  '，',\n",
       "  '她',\n",
       "  '不',\n",
       "  '時',\n",
       "  '以',\n",
       "  '港',\n",
       "  '區',\n",
       "  '人',\n",
       "  '大',\n",
       "  '常',\n",
       "  '委',\n",
       "  '的',\n",
       "  '身',\n",
       "  '份',\n",
       "  '公',\n",
       "  '開',\n",
       "  '發',\n",
       "  '表',\n",
       "  '談',\n",
       "  '話',\n",
       "  '，',\n",
       "  '又',\n",
       "  '接',\n",
       "  '受',\n",
       "  '傳',\n",
       "  '媒',\n",
       "  '專',\n",
       "  '訪',\n",
       "  '，',\n",
       "  '但',\n",
       "  '言',\n",
       "  '論',\n",
       "  '往',\n",
       "  '往',\n",
       "  '引',\n",
       "  '起',\n",
       "  '爭',\n",
       "  '議',\n",
       "  '，',\n",
       "  '甚',\n",
       "  '至',\n",
       "  '引',\n",
       "  '來',\n",
       "  '不',\n",
       "  '少',\n",
       "  '人',\n",
       "  '的',\n",
       "  '抨',\n",
       "  '擊',\n",
       "  '，',\n",
       "  '例',\n",
       "  '如',\n",
       "  '說',\n",
       "  '香',\n",
       "  '港',\n",
       "  '年',\n",
       "  '輕',\n",
       "  '人',\n",
       "  '敵',\n",
       "  '視',\n",
       "  '國',\n",
       "  '家',\n",
       "  '是',\n",
       "  '因',\n",
       "  '為',\n",
       "  '老',\n",
       "  '師',\n",
       "  '反',\n",
       "  '對',\n",
       "  '共',\n",
       "  '產',\n",
       "  '黨',\n",
       "  '，',\n",
       "  '又',\n",
       "  '呼',\n",
       "  '籲',\n",
       "  '年',\n",
       "  '輕',\n",
       "  '人',\n",
       "  '如',\n",
       "  '果',\n",
       "  '對',\n",
       "  '制',\n",
       "  '度',\n",
       "  '不',\n",
       "  '滿',\n",
       "  '大',\n",
       "  '可',\n",
       "  '離',\n",
       "  '開',\n",
       "  '香',\n",
       "  '港',\n",
       "  '。',\n",
       "  '但',\n",
       "  '就',\n",
       "  '算',\n",
       "  '是',\n",
       "  '這',\n",
       "  '樣',\n",
       "  '，',\n",
       "  '仍',\n",
       "  '然',\n",
       "  '無',\n",
       "  '阻',\n",
       "  '她',\n",
       "  '不',\n",
       "  '時',\n",
       "  '充',\n",
       "  '份',\n",
       "  '發',\n",
       "  '揮',\n",
       "  '毋',\n",
       "  '須',\n",
       "  '廣',\n",
       "  '大',\n",
       "  '香',\n",
       "  '港',\n",
       "  '人',\n",
       "  '投',\n",
       "  '票',\n",
       "  '授',\n",
       "  '權',\n",
       "  '，',\n",
       "  '仍',\n",
       "  '然',\n",
       "  '可',\n",
       "  '以',\n",
       "  '「',\n",
       "  '無',\n",
       "  '票',\n",
       "  '傲',\n",
       "  '物',\n",
       "  '」',\n",
       "  '的',\n",
       "  '超',\n",
       "  '然',\n",
       "  '身',\n",
       "  '份',\n",
       "  '。',\n",
       "  '當',\n",
       "  '然',\n",
       "  '，',\n",
       "  '在',\n",
       "  '中',\n",
       "  '國',\n",
       "  '特',\n",
       "  '色',\n",
       "  '的',\n",
       "  '民',\n",
       "  '主',\n",
       "  '集',\n",
       "  '中',\n",
       "  '制',\n",
       "  '，',\n",
       "  '大',\n",
       "  '部',\n",
       "  '份',\n",
       "  '香',\n",
       "  '港',\n",
       "  '人',\n",
       "  '也',\n",
       "  '心',\n",
       "  '知',\n",
       "  '肚',\n",
       "  '明',\n",
       "  '制',\n",
       "  '度',\n",
       "  '設',\n",
       "  '計',\n",
       "  '的',\n",
       "  '目',\n",
       "  '標',\n",
       "  '，',\n",
       "  '就',\n",
       "  '是',\n",
       "  '要',\n",
       "  '讓',\n",
       "  '當',\n",
       "  '權',\n",
       "  '的',\n",
       "  '中',\n",
       "  '共',\n",
       "  '可',\n",
       "  '以',\n",
       "  '不',\n",
       "  '受',\n",
       "  '挑',\n",
       "  '戰',\n",
       "  '。',\n",
       "  '至',\n",
       "  '於',\n",
       "  '那',\n",
       "  '些',\n",
       "  '人',\n",
       "  '大',\n",
       "  '常',\n",
       "  '委',\n",
       "  '或',\n",
       "  '人',\n",
       "  '大',\n",
       "  '代',\n",
       "  '表',\n",
       "  '是',\n",
       "  '否',\n",
       "  '真',\n",
       "  '的',\n",
       "  '可',\n",
       "  '以',\n",
       "  '當',\n",
       "  '仁',\n",
       "  '不',\n",
       "  '讓',\n",
       "  '為',\n",
       "  '民',\n",
       "  '喉',\n",
       "  '舌',\n",
       "  '、',\n",
       "  '監',\n",
       "  '督',\n",
       "  '政',\n",
       "  '府',\n",
       "  '，',\n",
       "  '絕',\n",
       "  '大',\n",
       "  '部',\n",
       "  '份',\n",
       "  '市',\n",
       "  '民',\n",
       "  '已',\n",
       "  '經',\n",
       "  '是',\n",
       "  '不',\n",
       "  '存',\n",
       "  '盼',\n",
       "  '望',\n",
       "  '了',\n",
       "  '。',\n",
       "  '范',\n",
       "  '太',\n",
       "  '這',\n",
       "  '一',\n",
       "  '次',\n",
       "  '的',\n",
       "  '發',\n",
       "  '言',\n",
       "  '如',\n",
       "  '果',\n",
       "  '有',\n",
       "  '正',\n",
       "  '確',\n",
       "  '之',\n",
       "  '處',\n",
       "  '，',\n",
       "  '可',\n",
       "  '能',\n",
       "  '只',\n",
       "  '是',\n",
       "  '指',\n",
       "  '出',\n",
       "  '了',\n",
       "  '近',\n",
       "  '年',\n",
       "  '香',\n",
       "  '港',\n",
       "  '社',\n",
       "  '會',\n",
       "  '的',\n",
       "  '對',\n",
       "  '立',\n",
       "  '加',\n",
       "  '劇',\n",
       "  '，',\n",
       "  '不',\n",
       "  '同',\n",
       "  '派',\n",
       "  '系',\n",
       "  '的',\n",
       "  '議',\n",
       "  '員',\n",
       "  '互',\n",
       "  '不',\n",
       "  '相',\n",
       "  '讓',\n",
       "  '。',\n",
       "  '表',\n",
       "  '現',\n",
       "  '在',\n",
       "  '議',\n",
       "  '會',\n",
       "  '的',\n",
       "  '操',\n",
       "  '作',\n",
       "  '過',\n",
       "  '程',\n",
       "  '中',\n",
       "  '，',\n",
       "  '透',\n",
       "  '過',\n",
       "  '分',\n",
       "  '區',\n",
       "  '直',\n",
       "  '選',\n",
       "  '的',\n",
       "  '部',\n",
       "  '份',\n",
       "  '民',\n",
       "  '主',\n",
       "  '派',\n",
       "  '的',\n",
       "  '議',\n",
       "  '員',\n",
       "  '與',\n",
       "  '建',\n",
       "  '制',\n",
       "  '派',\n",
       "  '的',\n",
       "  '議',\n",
       "  '員',\n",
       "  '關',\n",
       "  '係',\n",
       "  '越',\n",
       "  '來',\n",
       "  '越',\n",
       "  '緊',\n",
       "  '張',\n",
       "  '。',\n",
       "  '當',\n",
       "  '分',\n",
       "  '歧',\n",
       "  '變',\n",
       "  '得',\n",
       "  '如',\n",
       "  '此',\n",
       "  '嚴',\n",
       "  '重',\n",
       "  '，',\n",
       "  '大',\n",
       "  '家',\n",
       "  '都',\n",
       "  '無',\n",
       "  '面',\n",
       "  '畀',\n",
       "  '，',\n",
       "  '議',\n",
       "  '會',\n",
       "  '確',\n",
       "  '實',\n",
       "  '越',\n",
       "  '來',\n",
       "  '越',\n",
       "  '多',\n",
       "  '劍',\n",
       "  '拔',\n",
       "  '弩',\n",
       "  '張',\n",
       "  '的',\n",
       "  '場',\n",
       "  '面',\n",
       "  '，',\n",
       "  '正',\n",
       "  '常',\n",
       "  '的',\n",
       "  '立',\n",
       "  '法',\n",
       "  '及',\n",
       "  '各',\n",
       "  '種',\n",
       "  '體',\n",
       "  '現',\n",
       "  '議',\n",
       "  '會',\n",
       "  '憲',\n",
       "  '政',\n",
       "  '責',\n",
       "  '任',\n",
       "  '的',\n",
       "  '工',\n",
       "  '作',\n",
       "  '效',\n",
       "  '率',\n",
       "  '難',\n",
       "  '免',\n",
       "  '受',\n",
       "  '到',\n",
       "  '拖',\n",
       "  '慢',\n",
       "  '，',\n",
       "  '也',\n",
       "  '令',\n",
       "  '社',\n",
       "  '會',\n",
       "  '對',\n",
       "  '立',\n",
       "  '的',\n",
       "  '氣',\n",
       "  '氛',\n",
       "  '進',\n",
       "  '一',\n",
       "  '步',\n",
       "  '顯',\n",
       "  '現',\n",
       "  '。',\n",
       "  '出',\n",
       "  '現',\n",
       "  '這',\n",
       "  '一',\n",
       "  '種',\n",
       "  '現',\n",
       "  '象',\n",
       "  '，',\n",
       "  '真',\n",
       "  '的',\n",
       "  '只',\n",
       "  '是',\n",
       "  '部',\n",
       "  '份',\n",
       "  '新',\n",
       "  '晉',\n",
       "  '議',\n",
       "  '員',\n",
       "  '「',\n",
       "  '恃',\n",
       "  '票',\n",
       "  '傲',\n",
       "  '物',\n",
       "  '」',\n",
       "  '嗎',\n",
       "  '？',\n",
       "  '其',\n",
       "  '實',\n",
       "  '經',\n",
       "  '過',\n",
       "  '這',\n",
       "  '麼',\n",
       "  '多',\n",
       "  '年',\n",
       "  '的',\n",
       "  '爭',\n",
       "  '論',\n",
       "  '，',\n",
       "  '香',\n",
       "  '港',\n",
       "  '政',\n",
       "  '制',\n",
       "  '的',\n",
       "  '核',\n",
       "  '心',\n",
       "  '問',\n",
       "  '題',\n",
       "  '已',\n",
       "  '經',\n",
       "  '十',\n",
       "  '分',\n",
       "  '清',\n",
       "  '楚',\n",
       "  '，',\n",
       "  '首',\n",
       "  '先',\n",
       "  '就',\n",
       "  '是',\n",
       "  '作',\n",
       "  '為',\n",
       "  '最',\n",
       "  '高',\n",
       "  '民',\n",
       "  '意',\n",
       "  '機',\n",
       "  '關',\n",
       "  '的',\n",
       "  '立',\n",
       "  '法',\n",
       "  '會',\n",
       "  '，',\n",
       "  '仍',\n",
       "  '然',\n",
       "  '有',\n",
       "  '一',\n",
       "  '半',\n",
       "  '議',\n",
       "  '席',\n",
       "  '是',\n",
       "  '以',\n",
       "  '小',\n",
       "  '圈',\n",
       "  '子',\n",
       "  '的',\n",
       "  '方',\n",
       "  '式',\n",
       "  '產',\n",
       "  '生',\n",
       "  '，',\n",
       "  '其',\n",
       "  '中',\n",
       "  '有',\n",
       "  '十',\n",
       "  '多',\n",
       "  '位',\n",
       "  '議',\n",
       "  '員',\n",
       "  '是',\n",
       "  '完',\n",
       "  '全',\n",
       "  '毋',\n",
       "  '須',\n",
       "  '選',\n",
       "  '舉',\n",
       "  '便',\n",
       "  '自',\n",
       "  '動',\n",
       "  '當',\n",
       "  '選',\n",
       "  '，',\n",
       "  '部',\n",
       "  '份',\n",
       "  '更',\n",
       "  '是',\n",
       "  '在',\n",
       "  '議',\n",
       "  '會',\n",
       "  '多',\n",
       "  '年',\n",
       "  '在',\n",
       "  '冇',\n",
       "  '競',\n",
       "  '爭',\n",
       "  '下',\n",
       "  '一',\n",
       "  '再',\n",
       "  '連',\n",
       "  '任',\n",
       "  '，',\n",
       "  '議',\n",
       "  '政',\n",
       "  '水',\n",
       "  '平',\n",
       "  '低',\n",
       "  '下',\n",
       "  '，',\n",
       "  '也',\n",
       "  '不',\n",
       "  '見',\n",
       "  '得',\n",
       "  '有',\n",
       "  '甚',\n",
       "  '麼',\n",
       "  '建',\n",
       "  '樹',\n",
       "  '。',\n",
       "  '特',\n",
       "  '首',\n",
       "  '功',\n",
       "  '能',\n",
       "  '組',\n",
       "  '別',\n",
       "  '缺',\n",
       "  '民',\n",
       "  '意',\n",
       "  '授',\n",
       "  '權',\n",
       "  '早',\n",
       "  '在',\n",
       "  '主',\n",
       "  '權',\n",
       "  '移',\n",
       "  '交',\n",
       "  '之',\n",
       "  '前',\n",
       "  '，',\n",
       "  '社',\n",
       "  '會',\n",
       "  '上',\n",
       "  '已',\n",
       "  '經',\n",
       "  '有',\n",
       "  '廣',\n",
       "  '泛',\n",
       "  '的',\n",
       "  '共',\n",
       "  '識',\n",
       "  '，',\n",
       "  '就',\n",
       "  '是',\n",
       "  '要',\n",
       "  '逐',\n",
       "  '步',\n",
       "  '減',\n",
       "  '少',\n",
       "  '以',\n",
       "  '至',\n",
       "  '取',\n",
       "  '消',\n",
       "  '功',\n",
       "  '能',\n",
       "  '議',\n",
       "  '席',\n",
       "  '。',\n",
       "  '部',\n",
       "  '份',\n",
       "  '屬',\n",
       "  '於',\n",
       "  '建',\n",
       "  '制',\n",
       "  '派',\n",
       "  '的',\n",
       "  '政',\n",
       "  '壇',\n",
       "  '元',\n",
       "  '老',\n",
       "  '，',\n",
       "  '也',\n",
       "  '指',\n",
       "  '出',\n",
       "  '當',\n",
       "  '時',\n",
       "  '大',\n",
       "  '家',\n",
       "  '都',\n",
       "  '有',\n",
       "  '共',\n",
       "  '同',\n",
       "  '的',\n",
       "  '理',\n",
       "  '解',\n",
       "  '，',\n",
       "  '功',\n",
       "  '能',\n",
       "  '議',\n",
       "  '席',\n",
       "  '只',\n",
       "  '是',\n",
       "  '過',\n",
       "  '渡',\n",
       "  '性',\n",
       "  '的',\n",
       "  '安',\n",
       "  '排',\n",
       "  '。',\n",
       "  '根',\n",
       "  '據',\n",
       "  '《',\n",
       "  '基',\n",
       "  '本',\n",
       "  '法',\n",
       "  '》',\n",
       "  '，',\n",
       "  '立',\n",
       "  '法',\n",
       "  '會',\n",
       "  '議',\n",
       "  '席',\n",
       "  '也',\n",
       "  '應',\n",
       "  '該',\n",
       "  '逐',\n",
       "  '步',\n",
       "  '邁',\n",
       "  '向',\n",
       "  '全',\n",
       "  '面',\n",
       "  '普',\n",
       "  '選',\n",
       "  '。',\n",
       "  '但',\n",
       "  '主',\n",
       "  ...]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build the vocabulary. As this dataset is small (only ~3800 training examples) it also has a very small vocabulary (~7500 unique tokens), this means we do not need to set a `max_size` on the vocabulary as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can check the labels.\n",
    "\n",
    "The 6 labels (for the non-fine-grained case) correspond to the 6 types of questions in the dataset:\n",
    "- `HUM` for questions about humans\n",
    "- `ENTY` for questions about entities\n",
    "- `DESC` for questions asking you for a description \n",
    "- `NUM` for questions where the answer is numerical\n",
    "- `LOC` for questions where the answer is a location\n",
    "- `ABBR` for questions asking about abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x1190a2268>, {'hky': 0, 'zh': 1, 'en': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we set up the iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the CNN model from the previous notebook, however any of the models covered in these tutorials will work on this dataset. The only difference is now the `output_dim` will be $C$ instead of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model, making sure to set `OUTPUT_DIM` to $C$. We can get $C$ easily by using the size of the `LABEL` vocab, much like we used the length of the `TEXT` vocab to get the size of the vocabulary of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another different to the previous notebooks is our loss function (aka criterion). Before we used `BCEWithLogitsLoss`, however now we use `CrossEntropyLoss`. Without going into too much detail, `CrossEntropyLoss` performs a *softmax* function over our model outputs and the loss is given by the *cross entropy* between that and the label.\n",
    "\n",
    "Generally:\n",
    "- `CrossEntropyLoss` is used when our examples exclusively belong to one of $C$ classes\n",
    "- `BCEWithLogitsLoss` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over 0.5 then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class. \n",
    "\n",
    "For example, in our labels we have: 'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5. If the output of our model was something like: **[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]** this means that the model strongly believes the example belongs to class 0, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
    "\n",
    "We calculate the accuracy by performing an `argmax` to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum()/torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is similar to before, without the need to `squeeze` the model predictions as `CrossEntropyLoss` expects the input to be **[batch size, n classes]** and the label to be **[batch size]**.\n",
    "\n",
    "The label needs to be a `LongTensor`, which it is by default as we did not set the `dtype` to a `FloatTensor` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is, again, similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/keras/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 1.389 | Train Acc: 41.18% | Val. Loss: 0.934 | Val. Acc: 49.53% | Saved: True\n",
      "| Epoch: 02 | Train Loss: 1.062 | Train Acc: 50.00% | Val. Loss: 0.974 | Val. Acc: 41.61% | Saved: False\n",
      "| Epoch: 03 | Train Loss: 0.831 | Train Acc: 70.59% | Val. Loss: 0.982 | Val. Acc: 41.61% | Saved: False\n",
      "| Epoch: 04 | Train Loss: 0.929 | Train Acc: 55.88% | Val. Loss: 0.875 | Val. Acc: 41.61% | Saved: True\n",
      "| Epoch: 05 | Train Loss: 0.655 | Train Acc: 67.65% | Val. Loss: 0.761 | Val. Acc: 49.53% | Saved: True\n",
      "| Epoch: 06 | Train Loss: 0.600 | Train Acc: 64.71% | Val. Loss: 0.693 | Val. Acc: 75.16% | Saved: True\n",
      "| Epoch: 07 | Train Loss: 0.397 | Train Acc: 88.24% | Val. Loss: 0.670 | Val. Acc: 64.32% | Saved: True\n",
      "| Epoch: 08 | Train Loss: 0.539 | Train Acc: 73.53% | Val. Loss: 0.664 | Val. Acc: 55.23% | Saved: True\n",
      "| Epoch: 09 | Train Loss: 0.384 | Train Acc: 88.24% | Val. Loss: 0.644 | Val. Acc: 58.46% | Saved: True\n",
      "| Epoch: 10 | Train Loss: 0.383 | Train Acc: 88.24% | Val. Loss: 0.616 | Val. Acc: 64.82% | Saved: True\n",
      "| Epoch: 11 | Train Loss: 0.359 | Train Acc: 85.29% | Val. Loss: 0.582 | Val. Acc: 80.36% | Saved: True\n",
      "| Epoch: 12 | Train Loss: 0.247 | Train Acc: 94.12% | Val. Loss: 0.566 | Val. Acc: 90.13% | Saved: True\n",
      "| Epoch: 13 | Train Loss: 0.281 | Train Acc: 88.24% | Val. Loss: 0.572 | Val. Acc: 80.34% | Saved: False\n",
      "| Epoch: 14 | Train Loss: 0.150 | Train Acc: 97.06% | Val. Loss: 0.590 | Val. Acc: 68.91% | Saved: False\n",
      "| Epoch: 15 | Train Loss: 0.315 | Train Acc: 88.24% | Val. Loss: 0.584 | Val. Acc: 67.73% | Saved: False\n",
      "| Epoch: 16 | Train Loss: 0.127 | Train Acc: 97.06% | Val. Loss: 0.577 | Val. Acc: 68.13% | Saved: False\n",
      "| Epoch: 17 | Train Loss: 0.158 | Train Acc: 97.06% | Val. Loss: 0.557 | Val. Acc: 72.63% | Saved: True\n",
      "| Epoch: 18 | Train Loss: 0.158 | Train Acc: 94.12% | Val. Loss: 0.522 | Val. Acc: 78.49% | Saved: True\n",
      "| Epoch: 19 | Train Loss: 0.106 | Train Acc: 97.06% | Val. Loss: 0.487 | Val. Acc: 89.24% | Saved: True\n",
      "| Epoch: 20 | Train Loss: 0.119 | Train Acc: 97.06% | Val. Loss: 0.464 | Val. Acc: 92.47% | Saved: True\n",
      "| Epoch: 21 | Train Loss: 0.167 | Train Acc: 97.06% | Val. Loss: 0.446 | Val. Acc: 92.47% | Saved: True\n",
      "| Epoch: 22 | Train Loss: 0.113 | Train Acc: 100.00% | Val. Loss: 0.432 | Val. Acc: 92.08% | Saved: True\n",
      "| Epoch: 23 | Train Loss: 0.091 | Train Acc: 100.00% | Val. Loss: 0.422 | Val. Acc: 93.75% | Saved: True\n",
      "| Epoch: 24 | Train Loss: 0.087 | Train Acc: 100.00% | Val. Loss: 0.413 | Val. Acc: 93.75% | Saved: True\n",
      "| Epoch: 25 | Train Loss: 0.081 | Train Acc: 100.00% | Val. Loss: 0.406 | Val. Acc: 94.53% | Saved: True\n",
      "| Epoch: 26 | Train Loss: 0.043 | Train Acc: 100.00% | Val. Loss: 0.400 | Val. Acc: 92.86% | Saved: True\n",
      "| Epoch: 27 | Train Loss: 0.071 | Train Acc: 97.06% | Val. Loss: 0.393 | Val. Acc: 92.86% | Saved: True\n",
      "| Epoch: 28 | Train Loss: 0.052 | Train Acc: 100.00% | Val. Loss: 0.388 | Val. Acc: 92.47% | Saved: True\n",
      "| Epoch: 29 | Train Loss: 0.054 | Train Acc: 100.00% | Val. Loss: 0.383 | Val. Acc: 93.26% | Saved: True\n",
      "| Epoch: 30 | Train Loss: 0.066 | Train Acc: 97.06% | Val. Loss: 0.380 | Val. Acc: 93.65% | Saved: True\n",
      "CPU times: user 10min 9s, sys: 52.2 s, total: 11min 1s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "N_EPOCHS = 30\n",
    "\n",
    "lowest_valid_loss = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    # save the model with the lowest validation loss for use later\n",
    "    saved = False\n",
    "    if valid_loss < lowest_valid_loss:\n",
    "        lowest_valid_loss = valid_loss\n",
    "        with open(\"./models/language-identifier-\" + DATASET + \"-best.pt\", 'wb') as fb:\n",
    "            saved = True\n",
    "            torch.save(model, fb)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Saved: {saved}')\n",
    "    \n",
    "with open(\"./models/language-identifier-\" + DATASET + \"-final.pt\", 'wb') as ff:\n",
    "    torch.save(model, ff)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-fine-grained case, we should get an accuracy of around 90%. For the fine-grained case, we should get around 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.456 | Test Acc: 93.03% |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning tend to overfit the training data if it ran for too many epochs. We'll compare with the best model we've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/language-identifier-\" + DATASET + \"-best.pt\", 'rb') as fbl:\n",
    "    best_model = torch.load(fbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.456 | Test Acc: 93.03% |\n"
     ]
    }
   ],
   "source": [
    "best_model_test_loss, best_model_test_acc = evaluate(best_model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {best_model_test_loss:.3f} | Test Acc: {best_model_test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model with the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use final model.\n"
     ]
    }
   ],
   "source": [
    "if test_loss > best_model_test_loss:\n",
    "    print(\"Will use best_model.\")\n",
    "    selected_model = best_model\n",
    "else:\n",
    "    print(\"Will use final model.\")\n",
    "    selected_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we made a function to predict sentiment for any given sentences, we can now make a function that will predict the class of question given.\n",
    "\n",
    "The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1, we use the `argmax` to get the highest predicted class index. We then use this index with the label vocab to get the human readable label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence, trained_model, min_len=4):\n",
    "    tokenized = tokenizer(sentence)\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = trained_model(tensor)\n",
    "    print(preds)\n",
    "    max_preds = preds.argmax(dim=1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it out on a few different questions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7026,  1.6478, -2.2783]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 1 = zh\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"特朗普上周四（7日）曾表示，在3月1日達成貿易協議的最後期限前，他不會與中國國家主席習近平會晤。\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5305,  0.8855, -2.0936]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 0 = hky\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"喺未有互聯網之前，你老母叫你做人唔好太高眼角，正正常常嘅男人嫁出去就算。\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3669, -0.9359,  1.0961]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 2 = en\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"I need to get some food.\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_predictions(prelist, trained_model, min_len=4):\n",
    "    min_len = 4\n",
    "    predict_hky = {}\n",
    "    predict_zh = {}\n",
    "    predict_en = {}\n",
    "    for token in prelist:\n",
    "        tokenized = tokenizer(token)\n",
    "        tokenized_len = len(tokenized)\n",
    "        if tokenized_len < min_len:\n",
    "            tokenized += ['<pad>'] * (min_len - tokenized_len)\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        preds = trained_model(tensor)\n",
    "        max_preds = preds.argmax(dim=1)\n",
    "        if LABEL.vocab.itos[max_preds.item()] == 'hky':\n",
    "            predict_hky[token] = preds.data[0][max_preds.item()].item()\n",
    "        elif LABEL.vocab.itos[max_preds.item()] == 'zh':\n",
    "            predict_zh[token] = preds.data[0][max_preds.item()].item()\n",
    "        else:\n",
    "            predict_en[token] = preds.data[0][max_preds.item()].item()\n",
    "    return predict_hky, predict_zh, predict_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hky, predict_zh, predict_en = range_predictions(TEXT.vocab.itos, selected_model, min_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('成', 0.6932101249694824)\n",
      "('廁', 0.6346280574798584)\n",
      "('返', 0.566817045211792)\n",
      "('振', 0.5665998458862305)\n",
      "('な', 0.5618364810943604)\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(predict_hky.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('魷', 0.9234397411346436)\n",
      "('播', 0.8398351073265076)\n",
      "('奪', 0.8303267359733582)\n",
      "('不', 0.828819751739502)\n",
      "('向', 0.8213356733322144)\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(predict_zh.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_value = sorted(predict_en.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what kind of articles are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \"zh\" but should be \"hky\".\n",
      "Article: \n",
      "公務員啲工會點解冇理由講到自己咁偉大？\n",
      "\n",
      "Predicted \"zh\" but should be \"hky\".\n",
      "Article: \n",
      "「中央最信任的香港人」若果是梁愛詩，佢話「梁營眾叛親離」係代表乜嘢？\n",
      "\n",
      "Predicted \"zh\" but should be \"hky\".\n",
      "Article: \n",
      "相關文章：林<unk>光講咗又唔認　前年港視事件早有前科<unk>范中流反<unk>二副未得善終　林<unk>光講船員叛變故事為警告港人？<unk>皇<unk>清\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "在經典科幻電影《星球大戰》(StarWars)當中扮演「<unk>亞公主」（PrincessLeia）而聞名的女演員CarrieFisher，早前心<unk>病發入院，經過多日治療後在美國時間星期二（12月27日）逝世，享年60歲。CarrieFisher在12月23日從倫敦飛返洛杉磯的航機上，降落15分鐘前突發心<unk>病，心<unk>停止了跳動。機組人員和乘客中的醫生和護士為她進行了心肺復蘇緊急<unk>救。之後被送進醫院<unk>救，但終於在四天後去世。1956年10月21日出生在洛杉磯的CarrieFisher，母親是著名演員DebbieReynolds，父親是男歌手EddieFisher。CarrieFisher成名後，曾患上<unk>鬱症，又染上毒<unk>。廣告CarrieFisher曾於《90男歡女愛》（WhenHarryMetSally…）等電影演出，她同時也是小說家、劇作家。她寫的半自傳式暢銷小說 \"PostcardsFromtheEdge\" 被改編為電影，她擔任編劇。廣告\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "一個瘋狂時代，一座醜聞城市，一場政治鬥爭，血淋淋的針鋒相對......聽起來似曾相識吧？實際是「無用一號實驗室」即將於周日上演的作品《市長先生》。現實太多殘<unk>，有些事情我們希望重新嚟過，但不是事事都可restart。想要不如重新開始的，有前輩級的中大榮休教授陳育強，將數碼書法帶入<unk><unk>創作。生活很忙，「忙忙忙•忘記了」太多。香港<unk>畫師Wing<unk>Clayton的畫展，帶你找尋被遺忘的好東西。廣告[展<unk>]不如重新開始廣告日期：2017年8月18至9月8日時間：12:00-19:00（星期一至四）11:00-19:00（星期五、六）地點：巨年藝廊[展<unk>]旅人<unk>性別覺察 日期：2017年8月18至11月18日時間：11:00-19:00（星期五、六）地點：480.0性別x藝術空間[展<unk>]忙忙忙•忘記了日期：即日起至2017年8月20日時間：14:00-21:30地點：走馬<unk>[劇場]仲夏夜之夢2.0日期：2017年8月18至20日時間：15:00/19:45地點：<unk>青劇院演藝廳[劇場]與西西玩遊戲日期：2017年8月19至20日時間：15:00/18:00/21:30地點：浪人劇場[劇場]八月實驗《市長先生》日期：2017年8月20日時間：18:15/20:00地點：戲子館[文學]聲韻詩刊第36期詩會日期：2017年8月20日時間：19:30-21:30地點：香港文學生活館[音樂]阿<unk>日日期：2017年8月19日時間：14:30-17:45地點：青年廣場1樓Y展<unk>平台\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "（photoviaccFlickruserHannaIrisTolonen） 陳<unk>迅名曲《黑擇明》，很適合這個日日血案新聞動魄驚心的城市，這首歌由曾經自殺，現在大<unk>大悟的林<unk>勸別人珍惜生命，令本歌另帶救<unk>意味。「他不姓黑，不怕黑，選了光，叫最暗黑的戲院發出光，臨行仍不肯<unk>手，拍出一片彩色給仰望」歌名幽了東<unk>不<unk>名導演黑澤明一關，亦語帶多關——黑擇明，黑暗選擇光明，這幾句已經很有畫面，戲院當然無<unk>，黑暗中一點<unk>光，彷彿汪洋中一<unk>塔，導人向善。「他很有心，很會講，黑暗中老百姓怎麼發出熱與汗，人又有幾多怕光要急於往花<unk>下被探望」黑暗中掙扎求存的人猶如錦衣夜行，那片看不透的<unk>暗猶如扯着人們情緒掉落谷的觸手，詞人是過來人，明白抑鬱的人並不需要<unk><unk>的加油，而是世界的認同。「未夠色，便要<unk>，若有日你也開鏡，願對白不要認你命」我唔開心很<unk>統，我抑鬱症卻嚇親人，詞人以拍電影為喻，心病還需心藥醫，奈何這個社會心藥短缺，冇病鬱到病，有病卻要扮冇病，否則一街都係有色眼鏡，教你頓變癡線佬「別要驚，別要驚，亂世下佈滿樽頸，這都市已<unk>夠血<unk>」<unk>夠血<unk>的都市，皆因瘋狂新聞早已尋常：男人殺妻<unk>屍落街再跳樓自殺，一開學就教師學生<unk><unk>跳樓，某人渣帶朋友回家強姦自己妻子，寫出來都覺癡線的事，卻是今日香港眾生相。「情緒或高或低如此<unk>秘，<unk><unk>難講理，既然浮生就如遊戲，不如坐戰機」情緒病不單是一種病，藥石治標不治本，詞人用「<unk><unk>」比喻之，皆因情緒病發就似天氣，有時當真冇解，一味勸人唔好死，對方真<unk>唔懂回應你。「黑暗下<unk>落光明中演你，心能隨心揀戲，這時期演傷心戲，戲爛人未死」黑暗中光明<unk>落，表面上又是錦衣夜行，其實詞人在勸世——唯一中和情緒病的方法就是別再為別人的眼光而活，別再扭曲自己心性做返真我，或可不藥而<unk>。「失戀也死，走去死，走去死，你母親傷心到死內疚未？誰人<unk>不出<unk>主，似三歲跳飛機，悲夠未？」你條命的確貴客自理，但你的生死還會掀動許多愛你的人<unk>戚，詞人歌曲上半場對情緒病人說之以理，下半場則是動之以情，他沒有正能量撚上身一味<unk>目鼓勵病人，而是以三歲小<unk>跳飛機為喻——小<unk>玩遊戲玩得不好，跟情緒病雷同，全是不可抗力，何必太在乎？「抑鬱也死，想去死，想去死，你當你醫生已死，沒見地，忘掉了雙星報喜，把天<unk>當<unk>美地，<unk>未？」詞人故意<unk>低抑鬱症殺傷力——你抑鬱症就想死？世上大把慘事慘過抑鬱，其實這是曲筆，抑鬱症患者往往也是中二病重症，他以為心靈的痛苦就是他的全宇宙，其實他只是坐<unk>觀天<unk>，美化自己的痛楚容易，但這種傷春悲秋無病<unk><unk>對病情並無幫助。「誰也在暢讀死亡的筆記，不如來推推理：要求存似電玩遊戲，操練著戰機。」你的死亡筆記不屬於奇拿而是在你手上，人生如戲，也似遊戲，冇人天生係打機能手，你看別人<unk>灑<unk><unk>難關容易過，但別人台上一分鐘台下十年功你又知否，你的遊戲難易度就看你的<unk>鍊修為。「死也未怕又怕甚麼苦戲，不如重溫好戲，死亡遲早都找你，切勿憑自己。」你的戲，只有你可飾演一次，人總是會死的，在死神探訪你前，好好享受你的劇本吧。吹神加詞神交織出呢首<unk><unk>人口的《黑擇明》，以電影喻人生，把人心帶進電影院中，原來那是戲如人生，黑暗光明共濟，原來<unk>頭把黑暗過會是晨<unk>，只是閣下灰機太久冇為意，戲好戲爛，也請唔好死，有戲未為輸，如此苦世良言，遭<unk>東方一<unk>人心衰敗城市，人人身陷精神<unk><unk>，聽聽《黑擇明》說什麼，從黑暗中選擇光明。 則留言\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "精選提要Fotobeginner.com為一個分享攝影技<unk>及樂趣的網站，希望初學攝影的朋友享受拍攝之樂！無可否認，筆者在初學攝影的時候也常常犯錯，但最重要的是在錯誤中學習，這樣才會有進步！以下有10個攝影新手常犯的錯誤，你又有留意嗎？一起學習吧！(錯誤一)只用套裝鏡頭雖然套裝鏡頭已經可以拍出很多出色的相片(參考)，但普偏套裝鏡頭通常也只有18-55mm的<unk>距，而且最大光圈也只有f/3.5，但當你要拍攝的題材多了，便要考慮使用其他不同<unk>距、光圈的鏡頭，讓你有更多創作空間，例如拍出淺景深可以用50mmf/1.4，拍人像可以選擇85mm的<unk>距，拍微距可以利用100mm的微距鏡或裝上便宜的微距筒等。這個也是可換鏡頭相機的優勢呢！(錯誤二)永遠不用手動模式(M-Mode)雖然光圈先決(A/Av-Mode)、快門先決(S/Tv-Mode)等已經可以應用很多拍攝場景，但有一些題材如星空、光<unk>、夜景等，利用手動模式(M-Mode)可以讓你更有效地拍出想要的效果，而且當你要全面控制光圈、快門、ISO等組合時，也可以更了解它們之間的關係，從而對<unk>光有更好的了解。參考：學習使用手動模式(M-Mode)(錯誤三)用太多手動模式對於新手或業餘攝影師，其實不需只使用手動模式來拍攝，手動模式適合在光源變化不大的情況下使用，半自動模式如A/Av-Mode可以讓你拍攝更輕鬆，<unk>光更準確(配搭「<unk>光補償」按<unk>更好)，你專心做好構圖和尋找題材已經很好了！因此不要<unk>目追求「手動模式=懂攝影」啊！(錯誤四)不手動改變對<unk>點很多相機的預設對<unk>點模式是「自動選擇對<unk>點」，當你半按時相機會自動替你選擇應該對<unk>的位置，在日常的拍攝，其實這樣會令對<unk>錯誤的機率增加，最好的是把對<unk>點區域模式設定為「單點對<unk>」，這樣你可以準確知道要對<unk>的地方。記得半按對<unk>後不放，相機會自動鎖定對<unk>距離，這時你便可以移動相機構圖了！(錯誤五)不善用連續自動對<unk>功能(AF-C/AIServo)通常相機也會有3個自動對<unk>功能：單次自動對<unk>、連續自動對<unk>和兩者自動選擇，很多同學也只會使用「單次自動對<unk>」(AF-S/OneShot)而忽略了「連續自動對<unk>」(AF-C/AIServo)的功能，其實在拍攝一些會移動的東西（特別是向著或離開你）時，我們可以把自動對<unk>改為連續自動對<unk>模式，這樣相機會不斷自動對<unk>，加上連拍功能，你成功拍出清晰相片的機會便會大增了！參考：破解3種「自動對<unk>」功能，讓你拍出更<unk>利相片！(錯誤六)光圈太小光圈有兩個作用，一是控制光線進入的多<unk>，二是控制景深的深淺，如果你的光圈太小，你的相片可能會：<unk>光不足：如果不放慢快門速度或提升ISO相片模<unk>：手持時因光圈小而快門過慢相片清晰度下降：很多時同學會聽到收小一點光圈會令成像更<unk>利，但過度收小光圈反而會因為物理上光線散射，令清晰度下降，因此不要把光圈收得過小啊！教學：甚麼時候用「大光圈」和「小光圈」？(錯誤七)光圈太大跟上一點相反，有時同學會常常把光圈開到最大，這樣不但會令相片清晰度下降，而且也很容易因景深過淺而令相片變模<unk>。(錯誤八)永遠不使用三腳架三腳架是很值得使用的器材，無論是在白天或晚上也會有機會用到，很多新手也怕拿著三腳架拍攝，其實例如流水、星<unk>、車<unk>、夜景等，三腳架也可以幫你穩定相機，不但能拍出清晰的相片，更可以大大增加你的拍攝題材！參考：9個發揮三腳架最大潛力的使用技<unk>(錯誤九)只使用內置閃<unk>內置閃<unk>對於一般人像補光很方便使用，但同學們卻常常只使用內置閃<unk>，其實只要利用一支能改變方向的外置閃<unk>，把閃光打在天花、<unk><unk>上作反光，你的人像作品已經會大大不同！如果再加上無線<unk>控閃<unk>（飛<unk>），作品便可以有更多變化和效果，務必要試試啊！參考：離機閃<unk>(飛<unk>)的技<unk>原文刊於攝影入門作者：AlexTam (攝影入門Fotobeginner.com創辦人、總編)封面圖：SergioD.P <unk>Flickr\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "再不想上堂，有些堂，點都要上，例如finalyearproject。因為，唔上，就無學分，無學分，就畢唔到業，畢唔到業，就好煩。所以，我跟朱千雪、小清及一名<unk>生男子組成的小隊，每個星期點都要見一次面。原則上，個finalyearproject的玩法，是四名同學不斷開會，每個星期匯報一下進展。但以我當時的狀態，根本完全無心情跟任何人商量任何題目，所以，一開局，已經有共識，分工合作。我寫晒成份功課，個<unk>生男負責做powerpoint之類的工具，小清與朱千雪則負責present。我問他們，信任我嗎？認識我的兩個都說相信，餘下的<unk>生男知道唔使自己做，求之不得。就此下決定。一直相安無事，直到接近埋門一腳，朱千雪終於忍唔住，向我發炮。不是為了份功課，是為了我的態度。可能我一直當她是女神般看待，即使我有女朋友，Amber在我身邊也好，我都會交出一份敬意。到Grace出現了，我的情緒被撕裂得四分五裂，都忘記了怎樣可以不黑面。朱千雪了解我的狀況，明顯不滿：「你咁樣做，對Amber真係好唔公平。」「你都試過一腳踏兩船，你有乜資格話我呀？」我竟然發佢脾氣。「但係我無唔尊重人，無試過專登喺一個面前帶另一個出來走來走去，又無試過光明正大咁未搞掂舊一個就拖住新一個。」「我邊有拖呀？」「有乜分別呀？」朱千雪好少會咁大脾氣，我都唔明佢嬲乜，都唔關佢事。廣告結果，個finalyearproject，每次開會，都有兩個好黑面的怪人。兩年多前，我還在狂追朱千雪；估都估唔到有朝一日會好似仇人咁互<unk>。餘下兩個成員只好盡力搞氣氛，小清好明顯不是呢方面的高手，<unk>生男更差。情況非常<unk>硬。總算勉強頂到最後，我交出一份拎A-的功課，算對得住大家有餘啦。朱千雪仍然沒有高興起來。鄭伊健與梁<unk><unk>已不再是封面人物，但形象大受打擊。始終是香港人，明星偶像要神聖的，最好不結婚不拍拖最好不做愛，難怪郭<unk>城曾經話自己係處男，女星又個個爭住認係教徒。我當年當然是力排眾議支持鄭伊健與梁<unk><unk>的少數啦。尤其梁<unk><unk>，她還未正式入行，只是模特兒，我曾經在信和買了兩張明星照，回學校<unk>朋友，話俾人聽這是我的女朋友，又真係會有人信。這是發生在李嘉慧事件之前的趣事。對梁<unk><unk>，我從來有份偏好。見到她被圍<unk>，於心不忍。可惜我自身也難保，即使算做排解了Amber與Grace的困局，但名聲已敗壞，已經不再是那個人見人愛的大組長。廣告最慘是連朱千雪都好似放棄了我。當finalyearproject完結後，大家的關係便告終了。這三年間的感情，<unk>弱得有如在冰<unk>中取出一條冰冰雪條，不用幾分鐘，便消失得無影無<unk>。樓花也完全不見<unk>影，她發生了什麼事，我毫不知情。最後，我剩下的朋友，就只有小清一個。如果要為這三年大學生<unk>作個總結，都可以用一事無成來形容。朋友，勉強算做有一個，但小清是小清，我完全估計到她很快會有自己的家庭，然後安心做一個好太太好媽媽，給朋友，尤其像我這類朋友的時間，一年都未必有一日。女朋友，勉強算做有兩個，但大家都在辛苦經營，完全望不見希望。成績，差到只能說勉強畢到業。不過，這個最無所謂，如果三年後，仍然要拿我的學業成績來自吹自<unk>，我會覺得悲<unk>。而，很多政客，在畢業三十年後，還把自己在中學做過班長的事當作人生最重要的一個章節來宣傳。唉⋯⋯隔了一排沒有找Grace，不知是否因為朱千雪的態度令我有點難受，我又忍不住去找Grace傾訴。根本完全在將個關係<unk>搞<unk>複雜。「Grace吖，唔該。」「我係呀。」「你點呀？」「冇乜嘢。在家，<unk>。」「我來找你，好不好？」「你又話唔再見我？」「我<unk>住你，想搵你傾下<unk>啫。」「你係咪有事呀？」「沒什麼事，只是，就快畢業了，你會跟我影畢業相嗎？」「你夠膽同我一齊影咩？」「唔怕啦，還怕什麼，我都走了。我想至少要同你影一張吧。」「咁你使唔使我送個公仔俾你？你鍾意布<unk>狗？」「唔好啦，咁核突，都唔知邊個發明要帶個公仔去影畢業相。一定係啲賣公仔嘅人。」「你一定係有啲事發生咗嘅，話我知乜事啦，係咪Amber呀？」「唔係，不過，朱千雪無啦啦嬲咗我啫。」「哦，原來係你個女神嬲咗你。」「女乜鬼神喎，成幾年前的事啦。」「十幾年後，幾十年後，你都會照樣當佢女神㗎啦。」「我有冇咁長情呀？」「你唔係長情，係唔甘心。未得到手就係未得到手，未得到手就點會依依不捨，係咪吖？」我唔識答，我想誠實地去答一聲係，但今日的張國強已經無之前蠢得咁交關，喺適當時候，會懂得沉默。何況，如果根據Grace的說法，我更加放唔低，更加會當佢女神嘅，應該會係李嘉慧。咁，點講得出口呀？最終，我都係搵了Grace，傾起畢業後的前途。「其實，我而家已經可以搵定工啦，finalyearproject做完，我一個星期只係返一日學就夠，搵份長工都有可能得。」「好吖，反正你都係鍾意做嘢多過讀書。咁，你諗住搵份乜工呀？」「雜誌囉，做住先，最好就入到<unk>仔啦，好似最勁咁。」「《<unk>週刊》？」「係呀？你話好唔好呀？」「<unk>，做住先之嘛，佢肯請，咪得囉。」「咁又係，咁等我去火車站拎份《Recruit》先。」做住先，卻變成做做做做做住先。 作者facebook\n",
      "\n",
      "Predicted \"hky\" but should be \"en\".\n",
      "Article: \n",
      "（中譯<unk>本在下）ThecurrentRioOlympicshassadlybeenbrandedasoneoftheugliestinrecentmemoriesasfarastheOlympicspiritisconcerned,orthelackofit.We’vesofarwitnessedmorethanourfairshareoftheunsightlysideofthisglobalsportingevent.BeforetheGamesevenstartedtherewerealreadyrisinganti-RussiansentimentsduetodopingscandalsandthenChineseswimmerSunYangtoppedit,notbybeingcalleda“drugcheat”,buttheensuingretaliatorywarofwordshelaunchedagainsthisaccuserthereafter,whichwasutterlyshameful.Fortunately,theGames’mostcelebratedmedallistMichaelPhelpssavedtheGames’PRimagefromspirallingoutofcontrolwithhisgraciousacknowledgementofhisdefeatinthe100-metrebutterflytoyoungfirst-timeOlympianJosephSchooling,whobecameafanofPhelps’aftermeetinghimeightyearsago.WithsuchadramaticturnofeventsunfoldingattheRioGames,Icouldn’tpossiblystandonthesidelinesandleavetheGamessosoon.Therefore,IdecidedtodedicateanothercolumntotheRioGames.Afterall,ittakesplaceonlyonceeveryfouryears.Whynot?Firstoff,“savetheday”isausefulonetousetodescribesomeonewhoturnsabadsituationaroundandproducesgoodresultslikePhelpshimself.Anothersimilarphraseis“snatchvictoryfromthejawsofdefeat”.Translation:toturndefeatintovictory;simpleenough.OthersportingtermsthathavebeenadaptedtodailyEnglishuseis“jumpthegun”;thetermwasoriginallyusedintrackandfieldeventswhenacompetitorwouldrunbeforethestartingpistolwasshottosignifythebeginningoftherace.Nowadays,it’scommonlyusedwhensomeonedoessomethingtoosoon,usuallyimpulsively.Oneotherusefulphraseis“blowthecompetitionaway”meaningtocompletelyandutterlybeatyouroppositionwithoutachanceofmakingacomeback.Prettyeasytopicture,Iguess.Okayafewmoreinterestingsportingphrasesandseeifyoucanguesswhatsportstheyoriginatedfrom.“Skatingonthinice”?Tooeasy?Itsmeaningisalsoprettyliteral,whichmeansdoingsomethingandputtingoneselfinaverydangerousposition.Howabout“ashotinthedark”?Ifoneistoldnottodosomethingbecauseit’slikeashotinthedark,itmeansthepersonisdoingitwithoutsufficientinformationorknowledge,whichise<unk>uivalenttofiringashotinthedark.Okay,whataboutthisone,“hitbelowthebelt”?Whatsportcoulditbe?Yes,yougotit,it’sboxing.Ifyouhitbelowthebeltmeanstodosomethingnottotallylegalorappropriate,likepunchingyouropponentbelowthebeltduringaboxingmatch.Welldoneguys,you’vewonthisonehandsdown,butbeforeIgo,Iwillchallengeyoursportingspirit.Forthenextfewdays,useasmanysportingphrasesaspossibleandtrytoimpressasmanyfriendsandcolleaguesaspossible.Ifyouhavebeenpayingattentiontomycolumnsandpractising,thenIthinkwemaybeontoawinner.Gomakemeproud,champ<unk>Morenextweek.今屆里約奧運是近年最不堪入目的一次，令人心痛之處是，其奧運精神竟至<unk>然無存。到目前為此，我們目<unk>了太多這件體壇盛事不太光<unk>的一面。拜俄羅斯選手服用興奮劑的醜聞所<unk>，早在比賽開始前，國際間已<unk><unk>起一股反俄情緒。孫楊繼而「再下一城」，但並非其禁藥風波所致，而是他對指控者具報復性、極為<unk>臉的回話。幸好，比賽最著名的得<unk>者<unk>比斯，對曾於八年前見面、如今打敗他的小粉絲<unk><unk>首登奧運即在100米<unk><unk>奪金的新加坡<unk>將史高<unk>（JosephSchooling）的親切表現，成功扭轉奧運近乎失控的公關形象。隨著這戲劇性轉變在里約奧運上演，我再不可能只站在場邊，這麼快就離身離去！所以，我決定再次把專欄獻給奧運，畢竟它也只是四年一度，何樂而不為？首先，「savetheday」形容像<unk>比斯般將劣局扭轉形勢、把爛<unk>子收<unk>妥當的行為。另一個意思相近的片語是「snatchvictoryfromthejawsofdefeat」，解作反敗為勝，夠簡單吧？「Jumpthegun」亦是一個在日常英語廣泛使用的運動術語，它本來指田徑項目中的「偷步」行為，但今時今日，我們可用來形容別人過早或太衝動地做事。另一有用的片語是「blowthecompetitionaway」，代表完完全全、徹底地打敗對手，對方連一絲還擊的機會沒有，從字面就很易想像。「Ashotinthedark」又如何？如果某人被叫不去做某事，因為有如「ashotinthedark」，意思就是他因知識、資料不足，舉動有如在黑暗中開槍。「Hitbelowthebelt」又與哪種運動有關呢？對，就是<unk>擊。如果選手向<unk>帶下方的位置出<unk>，代表你的行為不合規矩、不合適。好了，你們已輕易取勝了！但在文章完結前，讓我再挑戰一下你的體育精神吧！接下來的幾天，嘗試用盡可能多的體育片語，給你的朋友、同事留下深刻的印象。如果你有留心我的專欄，加以練習之下，相信不難取勝呢。讓我驕傲一番吧，下周再談！\n",
      "\n",
      "Predicted \"hky\" but should be \"en\".\n",
      "Article: \n",
      "Sharethis:(Eng.summarybelow)《開罐Opener》近日訪問了約三十名小學生，（https://www.facebook.com/OpenerHK/videos/10216292218007677/），發覺其學校不單普教中，甚至用普通話教其他科目，例如數學。這些兒童於是覺得香港粵語並非學習知識的語言（「普通話用來教學，廣東話用來鬧人」），日常即使講粵語，都會用普通話的詞<unk>（如「冷氣」變「空調」；「扮晒嘢」變「裝B」，按：B/逼乃<unk>口<unk>的<unk>音）；情況漸似今日的廣州。今日的廣州人，覺得粵語難登大<unk>之堂，大部份不<unk>講粵語，即使識講些少粵語，都多用普通話詞<unk>（如冰<unk>變雪<unk>），而且用普通話句式（如食<unk>佢講成把它<unk>光），有普通話口音（如<unk>士讀<unk>士，轟轟烈烈讀兇兇烈烈）。一種語言，一套世界觀（LeraBoroditsky2010）。香港學校繼續以普通話為教學語言，代表香港靈魂的香港粵語好快淪亡。應對方法係大家日日夜夜講香港粵語，用香港粵語創作，大力<unk><unk>普教中，並且質問教育局，所謂兩文三語教育政策，究竟為香港粵語教育做了些甚麼？“Opener’,aHKwebsite(https://www.facebook.com/OpenerHK/videos/10216292218007677/),recentlyinterviewed30primaryschoolstudents,andfoundthatasaresultoftheirschools’adoptingPutonghuaasthemediumofinstruction,theythinkthat“Putonghuaisforteachingandlearning,whileCantoneseisforswearing”,that“whenChinesetextsarereadinPutonghua,itsoundsmoreagreeable.”Intheirdailylife,evenwhentheyspeakCantonese,theywouldusevocabularyofPutonghuaratherthanthatofCantonese,e.g.,forSashimi,theywouldsaysaang1jyu4生魚[saang1jyu4literallymeanslivingfishbutitcanalsomeanblotchedsnakehead]insteadofjyu4-saang1魚生[fishalive];formotorcycle,theysaymo1-tok3-ce1<unk><unk>車[transliteration]inplaceofdin6-daan1-ce1電單車[electricbicycle];forshowingoffpompouslyandstupidly,theysayzong1-bi1裝逼[pretendcunt]ratherthanbaan6saai3je5扮晒嘢[tomega-pretend].ThesituationisgraduallyapproachingthatofGuangzhou,China,wheretheCantonesepeople,after70years’educationinPutonghuainsteadoftheirmothertongue,Cantonese,regardCantoneseasalow-classlanguage.MostofthemdonotspeakCantonese.EvenwhentheyspeaksomeCantonese,theyusePutonghuaphrases(e.g.,forrefrigerator,theyusebing1-soeng1冰<unk>[ice-box],insteadofsyut3-gwai6雪<unk>[snowcabinet]),Putonghuasyntax(e.g.,“Iwalk,comparedwithyou,faster”insteadoftheCantonesesyntax,“Iwalkfasterthanyou”),andevenwithPutonghuaaccent(e.g.,“seoi6-si6<unk>士Switzerland’hasbecome“jeoi6-si6<unk>士Switzerland”).Differentlanguagesleadtodifferentworldviews(LeraBoroditsky2010).TheeducatiionpolicyofteachinginPutonghuaisacolonialbrain-washingstrategyonthepartoftheChinesecommuniststodestroyHKCantonese,thesoulofHongKong,themothertongueofmostHongkongers,onethedefactoofficialspokenlanguagesofHKfor170years.IfHongKongschoolscontinuetoteachinPutonghua,HKCantonesewilldeterioratevery<unk>uickly(cf.RobertBauer2002).ThesolutionistospeakHKCantonesenightandday,tocreategreatworksinHKCantonese,tovigorouslyopposetheeducationalpolicyofteachinginPutonghua,andto<unk>uestiontheHKeducationalauthoritieswhathavetheydoneforHKCantoneseeducation,whentheyoftensay,they“aimtoenableHKstudentstobecomebiliterate[writtenChineseandEnglish]andtrilingual[spokenCantonese,EnglishandPutonghua].”普教中係殖民洗腦教育，上述小學生的「老師話課文用普通話讀會好聽啲」、「班主任話識多啲普通話同其他人講嘢方便啲」。這些未來的香港成年人，日常即使講粵語，都會用普通話的詞<unk>（例如：「電單車」變「<unk><unk>車」；行雷講打雷，按：打雷係北方俗語，行雷係古文<unk>言：《藝文類聚•雨》重雲吐飛電，高棟響行雷；電單車變<unk><unk>車；魚生刺身變了生魚片，按；廣東文化中，生魚係<unk>魚的別名）；普通話的句式（如我行路快過你，講成我行路比你快）；情況漸似今日的廣州。今日的廣州人，覺得粵語難登大<unk>之堂，大部份不識講粵語，即使識講粵語，都係在家中同不識字的長者講，而且用粵語講普通話的詞<unk>、普通話的句式（如食<unk>佢講成把它<unk>光）、甚至有普通話口音（如<unk>士讀<unk>士）。專門研究香港粵語的語言學<unk>授包<unk><unk>（RobertBauer）（二零零二）預測：普通話成為教學語言後一兩代間，香港學生用粵語正式學讀書面漢語之傳統將喪失，粵文書刊出<unk>會<unk><unk>，後生仔女將以粵語為<unk>，只在家中同老人家講，正如今下的廣州，而九七後大陸人不斷湧入香港，更會化廣東話做本港少數派語言，到本世紀下<unk>，粵語甚至會<unk>種。余以為應對方法係大家日日夜夜講香港粵語，寫文白粵交融的香港<unk>言，創作高水準的香港粵語文學，大戲，港產片，電視電臺節目，翻譯劇，原創劇，香港粵語流行曲，編寫香港粵語辭典（如粵辭正典），粵英字典。香港家長覺醒，反普教中，皆因知曉一旦形成講普通話的環境，仔女將來的飯<unk>就會被北方殖民<unk>走，事關你的普通話怎好都好不過大陸人。大中小學要增設香港粵語文化科，承先啟後本土文明。立法會議員要反對資助普教中，爭取香港粵語教學及研究資助，本土電影資助。市民見到教育局的官員，要問其所謂兩文三語教育政策，究竟對香港粵語教育有何貢獻？例如有無教香港粵語拼音法？有無推行香港粵語文化教育？Sharethis:Comments\n",
      "\n",
      "Predicted \"hky\" but should be \"en\".\n",
      "Article: \n",
      "LookattheformulaIdesignedasthispost’stitle,andpleaseagreeorrebukeme.Youknowhowthemediapicturestheideathatwhenyouarealoneinafestivetime,youfeelevenmorelonelythanever,andbeingsinglemeansanextralayeroflonelinessisaddedontopforyou.SoIcapturedaprint-screenofthisHongKongpsychologicalmagazineIwasreadingearlierthisweek(seebelowandsorryIonlyhavetheChineseversion):ittalksaboutaman’scasewherehedevelopeddepressionduetothefactthathewasdumpedbyhisex-girlfriendonthedayofChristmasEve,andthefestiveatmosphereofChristmasmadehimfeelevenworse;heusedtospendhisChristmaswithher.看看我為這篇文章標題設計的公式，你會同意或是反對呢？你知道的，媒體往往會<unk>述當你獨自一個過節日的時候，你會感到比任何時候都要孤獨，而單身意味著加了一層額外的孤獨感。所以我像在本週早些時候看到的一本香港心理學雜誌的文章cap了圖（見cap圖，對不起，我只有中文<unk>）：它談到一個男人的病例，他的抑鬱症誘因是他之前被前女友在聖誕前<unk>的那天甩了，而聖誕節的節日氣氛使他感覺更<unk>;他可習慣了和她一起度過聖誕節呀。Next,Iwanttoposea<unk>uestion:ifyouwerehim,wouldyoufinditshamefulifyouhavetoadmitthatbeingdumpedcausesyoutoturnintoamentallyillperson?接下來，我想提出一個問題：如果你是他，如果你要去承認因為前女友<unk>棄你導致你變成一個精神病人，你會覺得自己好<unk>愧嗎？Christmastimewouldbethebackgroundinformationwehave.Let’sbreakdownwhatIaskinparts.Firstthingfirst,isitshamefultofeeldepressiveifapersonisaloneinafestiveseason,likeChristmas?Thisoneiseasytoanswer,doesn’tit?Wheneverywhereisfilledwithanatmosphereinwhichpeoplespendtimetogether,whetheraslovers,familygatheringsorfriendshangingouttogether,itisveryreasonabletofeelmorelonelythanusual.Soalthoughbeingsinglemayfeelmorelonely,singlepeoplearen’tlonelyiftheyarespendingtheirChristmastimewithfriendsorfamilymembers.聖誕節將是我們的背景資訊。讓我分解我問的問題。首先，第一件事是，如果一個人獨自一個過節，如聖誕節，感到抑鬱是不是好<unk>愧呢？這一個部份很容易回答，不是嗎？當到處充滿了人們聚在一起的氣氛，無論是戀人們約會去，家庭聚會或是朋友們一起外出，感覺比平常更孤獨是非常合理的。所以雖然單身可能會感到更孤獨，如果單身人士與朋友或家庭成員度過聖誕節的時間，他們不用感到孤獨的。Next,soisitshamefultohavedevelopeddepressiontriggeredbythepainfulabandonmentbyaex-lover.ThisremindsmeofsomethingthatIwastaughtasakid.IrememberwhenIwaslittle,theadultsinmyfamilytoldmethatyoungpeopleshouldn’tdatetillweenteruniversity,becauseusteenswouldeitherbetooindulgedinthesweetnessofarelationship,orunabletomoveonintimewhenabreakupcomes.接下來，我想問因為被舊情人<unk>棄的痛苦經歷而引發了抑鬱症又是不是好<unk>愧呢？這讓我想起我小時候的教導。我記得當我還小的時候，我家裡的大人告訴我，除非進入大學，年輕人不應該談戀愛約會，因為我們青少年會太沉迷於這一種<unk><unk>關係，或者分手時無法及時抽身。Itdosenotmatteriftheoutcomeoftherelationshiptasteslikehoneyorforcesyoutoshedtears,itisaboutwhetheronecanhandleitwellandtoensurethatlifegoesonsmoothly.Umm,let’ssay,yougetupsetbygettingdumpedjustfewdaysbeforeyourexamsoyouscrewyourexamsup.However,itdosenottrulymakesense,doseit?Atuniversityyoucouldhavetogothroughthesamethingandscrewupyourexams,oratworkyoucouldscrewuptooandyourbosswouldmakeyouredundant.Itisabouthowmuchyourex-lovermeanstoyouastomakethiskindofimpacttoyoureverydaylife.無論關係的結果是<unk>點的<unk><unk>，或是迫使你流眼淚，說到底，這是關於一個人可不可以處理好失戀這回事，確保自己繼續生活順利。<unk>，例如說，你因為在考試前幾天被甩而感到不安，所以你的考試「炒粉」了。然而，這是真相的全部嗎？在大學，你可能需要經歷同樣的事情，你的考試表現被影響了，或你都上班了但影響了工作，你老闆會「炒魷」。你前情人對你的意義是甚麼，你的日常生活所產生的影響就自然會是甚麼。Itisindeedmelancholicbutromanticthatsomeoneloveshisexsomuchtotheextentthathegetsdepressionasaresultright?Iappreciatethatsomeonehastreatedarelationshipseriouslyevenheorshegetsdumpedeventually,justlikeweareencouragedtoalwaystryourbestinmanythingsinlife,butdon’tgetmewrong,Iwouldneverencourageanyonetogetdepressionasaconse<unk>uenceofapainfulbreakup.有人愛他的前任那麼多，以至於他得到抑鬱症，不是好<unk>鬱不過浪漫嗎？但就像我們在生活中總被鼓勵要我們嘗試做到最好，我<unk><unk>會認真對待一個感情關係的人，雖然他／她最終被甩了也不緊要。但不要誤會我，我永遠不會鼓勵任何人因為失戀而去患上抑鬱症。BacktothemanIwastalkingaboutthen.Isheguiltyofshametobebroughtdowntotheextentofenteringintoadepressivecondition?Iwouldhavesaidyesbefore.Ihavealwaysbelievedthatanyone,soevenanon-mentallyillpersonwhocriesforabreakupisapersonwhodisappointshimself,lackinghighaimsinhisownlife,andIlookeddownonmyselfsosomucheachtimewhenIeachtimecriedforanyreasons.IhatetoadmitthisbutlikemymyfamilyjokesandsaysthatIamverycapableofcryingsincethedayIwasborn,IhavetoomanyunhappymomentsbecauseIassociatedcryingwithself-devaluation.Onlyrecently,Istarttothinkslightlydifferently:amIstilldisappointingmyselfifIwouldgetmyselftogethereachtimeaftercryingandkeeponachievingthoseaimsofmine?回到那個我在談論的男人。他因為失戀而陷入抑鬱狀態的程度是不是好<unk>愧呢？我以前會說是的。我一直相信任何人，即使是一個非精神病患者，因為分手而<unk>泣是一個教自己失望的人，一個在自己的生活中沒了大志的人，所以，每次當我看著自己因為不同的原因<unk>起來，我都看不起自己。我不願意承認這一點，但像我的家人老是笑話說，我打從出生的那一天就好會<unk>了，我就有太多的不快樂時候，因為我把<unk>泣與自我<unk>值連繫了。只是最近我才開始有點不同：，如果我每次都在<unk>泣後讓自己在去繼續實現我的目標，我是不是還是教自己失望呢？Isupposeit’sapsychiatristwhohadtohelpthismantogetoverhisbreakup,andIimagineabitlikecuringaPTSD(post-traumaticstressdisorder).Iknownothingaboutpsychiatry,butcommonsensecanpointoutthatthebreakupisjustoneofmanytriggeringfactors,andevenitseemstobelittleaman(orawoman)whenwehearhimgettingdepressionforsuchareason,ratherthanforsomething<unk>big’likelosingajobordeathorsomeone.Iwouldhavetoreiteratethatitisnotsomeonechoosestobetriggeredbyaspecificfactor.Foreachdepressionpatient,acertainfactorhappenstoappearinhislifetoleadhimtosufferfromdepression.It’seasiertounderstandifwethinkabouthowweacceptthat<unk>uiteoftenthecauseofsomeonesufferingfromcancerisalsounknown.IthinkIunderstandwhyjobslikemarriagecounsellorsbesidespsychiatristsandtherapistsexistinthepsychologicalfield.我想是透過一個精神病醫生的治療，那個男人就走出失戀的<unk><unk>吧，我想像成像治<unk>PTSD（創傷後遺症）一樣。我對精神病學一無所知，但是常理可以指出，失戀只是許多觸發抑鬱症的因素之一，雖說當我們聽到一個男人（或一個女人）因為這樣的原因而感到抑鬱時，似乎他們被<unk>低了，因為可能我們會想，應當是一些比較「高價值」的原因，像失去工作，有人死亡才會讓人患上抑鬱症吧。我必須重申，不是人可選擇某個特定因素去觸發自己變成患者。對於每個抑鬱症患者，一定的因素恰好出現在他的生活中，導致他患有抑鬱症。如果我們想想我們如何經常接受<unk>症患者患<unk>的原因未能解釋，這就會更容易理解抑鬱症病人。我想我明白為什麼除了精神病醫生和治療師，婚姻輔導員這樣的工作存在於心理領域。Youwillrealiseafterall,Iamwritingtotalkaboutmyself.IamtalkingabouthowmuchIusedtolookdownonmyselfbecauseIeasilyburstedintotears,Ifeltsodepressivewhenmyex-loverleftme,andIwouldfeelshamefulbecauseIstronglybelievedthatbycryingalotduetomanyothersreasonstogetherwiththebreakup,Iamnothingbutacoward.Iamworkingonhowtostopcallingmyselfacoward,apieceofrubbish,thingslikethat.你會意識到，最後，我在談論我自己。我在說我有多麼看不起自己，因為我很容易<unk>，還有前情人離開我時我感到多麼抑鬱，我感到好<unk>愧，因為我堅信，由於失戀或許多其他原因而<unk>，我就是一個<unk>弱的人。我正在努力如何停止給自己與<unk>弱，一塊垃圾這些東西<unk>上等號。SonowIhavewrittenupthispostontheChristmasEve,andIjustwannasay,singleornot,lonelyornot,myformulaisacompletebullsh<unk>tbecauseyoucancertainlyfindyourjoyinyourownmeans.Iamtryingtocreatejoyformyself.ItellyounowwhatIamgoingtodoonthe25th:Iamvolunteeringagain,maybedoabitof<unk>peoplewatching’inacafétoreadafterwards(myexamsarecomingup…)andbytheendoftheday,justgohomeforasimpledinner.現在是平安夜的時間，我只是想說，單身又好，獨自一個又好，我那公式一定是bullsh<unk>t，因為你可以用你的方法找你的快樂。我在為自己制造快樂。我現在告訴你我25號會做甚麼好了：我又去做義工啦，可能之後去找個café去看看人來人往的景象再看看書（我快考試了。。。。。。），而再晚些就回家去<unk>一頓普通的晚飯吧。MerryX’mas.聖誕快樂。P.S.Atinypieceofinterestinginformation,seethepictureandpleasedon’tmixupmistletoeandhollyeveragain.有個有趣的小資訊，看看圖，其實不是聖誕花在英文有兩種說法的啦，但中文，「聖誕花」就是「聖誕花」吧！\n",
      "\n",
      "Predicted \"hky\" but should be \"en\".\n",
      "Article: \n",
      "JasonChow's SpeechonHongKongNationalParty-“DefendforDemocracy,HongKongIndependence”RallyTranscribedbyWilliam,translatedbyWilliamandSidney,spokenbyJasonChowHo-fai[TheEnglishtranslationisreleasedunderCreativeCommons,CCBY-NC-ND2.0](Source:Undergrad,HKUSUInstantNews)Helloeveryone,IamthespokespersonoftheHongKongNationalParty.Well,ontherallylastFriday,ImistakenlysaidthatIwastheconvener-actually,Iamthespokesperson,alright.MynameisChowHo-fai,sotodaywehavesuchalargeandpeacefulgathering;manyweredispleasedwithpeacefulgatherings,orperhapsdoubtful,andthatisbecausethePan-Democraticcamphasforthelastnineteenyears,s<unk>uanderedeachandeverychancewhenwehadone.Peacefulassembliesweresupposedtobeawaytogatherthecrowd,suchthatwecanwaitforanopeningtoputupafightonthestreetsortolaunchanoperationagainstthegovernment.YetthePan-Democratsdismissedthecrowdandsendeveryonehomeattheboilingpoint,timeaftertime.Manyagreatopportunityneverhavethechancetobecomesomething.BeforeIsaywhatIamsupposedtosaytoday,letmetalkaboutourpublicitymaterials.Itrustthateverybodyheremusthavereceivedleafletsonthefootbridgeoverthere,whenyoucameherefromtheAdmiraltyCentre.Ononeside,itjustsays“Independence<unk>”followedbyanexclamationmark.Sowhatisitallabout?Ifyouareholdingone,youcantakealookrightnow.Actually,itliststhedoubtsthatmanyHongKongpeople,manynormalcitizenshave:howwoulditbepossibleforHongKongtobecomeindependent,orspecifically,whydoesHongKonghavewhatittakestobecomeanindependentsovereignstate.Theyalwayssay,“ifwedeclareindependence,thenyouwon’thaveawatersupply,andyouloserswouldstarve”.Thatisnottrueactually,HongKongdoesnotrelyonChinamuchintermsoffoodsupply.Infact,Chinaisthebiggestfoodimporterintheworld.Asforfreshwater,SingaporeutilizesdesalinationandHongKongcanfollowtheirexample.ManysaidthattheyaregoingtoletloosethePLA,butthentheydidnotdothatonthe28thofSeptember,theyjustcalledthepolice.Whydidtheynotdothatisimportant-HongKongisaplacethatholdsalotofcapitalandpropertyforXiJinping,hisfamiliesandmanymoreCCPhigher-ups,astheyshifttheirassetsoutofChina.Assuch,suggestingthatthePLAwouldmarchoutandpurgeHongKongispreposterousandIdonotbelievethatitcouldhappen.ThatPLAbarracksoverthere,don’thavethewrongideathatithousesawholebunchoftough,seasonedwarriors.TheonesaregarrisonedinHongKong,aretheoneswhogotherevia“guanxi”(specialrelationships).BecauselifeintheHongKongGarrisoniswonderful,itisnottaxingatall.ShallweHongkongersbeafraidofthem?Ofcoursenot.Whyhaven’tHongkongersmakeabreakthroughpoliticallyalltheseyears?It’sbecauseourgreatestfearlivesinusourselves.Hongkongerslackthecouragetofacetheirfears.Theythinkthatwhoevermakesthefirstmovewillbesuppressed,orevenhavetheirpropertyraidedandconfiscated.ThisfearcamefromtheJune4thincident,whentheoldergenerationswitnessed,eitherontelevisionorattheTiananmenS<unk>uare,thePLAslaughteringthepeoplewithtanksandarmies.Then,ofcourseIshouldbetalkingabouttheconditionsthatmakeHongKongindependencepossible.IwillnowstartwhatIreallywanttotalkabouttoday.Hongkongershavefacedmanydifficultiesandhardshipsinthesenineteenyears.EversincetheHandoverin1997,Hongkongerswentfromfightingforhighidealssuchaspoliticalrights,tostrugglingforbasicsubsistencelikelivingspace.Whatwearefacingtoday,whatmanyHongkongersarefacingtodayisnotjustthelossofpoliticalrightsorfreedom,butanexistentialthreat.YouHongkongerswouldfinditextremelydifficulttorentaflat,andevenanicheforyourasheswouldbeahardtocomeby.It’shardenoughtosortoutthefuneralritesinHongKongafteryoudie,nottomentionactuallylivinghere.Wehavegonefromthepursuitofdemocracyandliberty,tostrugglingforbasicsurvivalrightsandspace.ThenwhatmadeHongkongersliveinagonyforthelastnineteenyears?Itwasthatfatefulnightin1997,whenHongkongersandHongKongdidnotchoosearoadtoindependenceandself-sufficiency,buttheychose,orrather,thoseso-calledintellectualsandeliteschoseforallHongkongers,andalsothenextgeneration,toreturntoChina.Theythinkthat“democraticreturntoChina”istherightthingtodo,theythinkitistheonlyproperandjustcourse.AndsoHongkongers,includingourgeneration,andthefuturegenerations,havetobeartheconse<unk>uences.Wehavetostruggleforourownmostbasicrighttoliveandbasicfreedoms.ActuallywhatIamtryingtosayisHongkongersabsolutelyhavetheabilitytogovernourselves.IbelieveHongkongersdopossesstheabilitytocreateahappysociety,andIbelieveHongkongershavetheabilitystandamongsttheWestorJapan,amongstthesecountriesandnotbefoundwanting.YetwhydoHongkongerstodayhavetoworryaboutwhetherournominatedcandidateswouldbedis<unk>ualified,andfeelgraciouswhentheyaren’t?Whyhavewesunksolow?ManypeopleclaimthatthereissomethingwrongwithHongKong,butno,HongKongisfine.AlltheproblemsarewiththeChinese,Chinaisthecruxoftheproblem.TheChinesegovernmentistheproblem,notyouHongkongers.Hongkongershavewastedtoomuchtime.Youhavedonenothingatallinthesenineteenyears.Bynothing,Imeanthatallyouhavedoneisparticipatinginpeacefulgatherings-thereisnothingwronginherentlywiththegatherings,butyougainednothingvaluablefromthem.YoudidnotstarttoorganizetheresistancebackthenwhenHongKongfelltoChina.That’swhatwentwrong.Wehavestartedlate,weneedtocatchup.Many<unk>uestionedhowcouldwepossiblyrealizeHongKongIndependence,ortheysayordinarycitizenswouldnottakeheedbecausetheyvaluetheirjobmorethantheirrights.Myresponseisthis:everyHongkongerhavetheirownparttoplayinfurtheringthegoalofHongKongIndependence.Everyonecandosomethingwithintheirownabilitiesandpositions,likeHitsujikojustsaid-ifyouarepartofthemiddle-class,youcanmakemonetarycontributionsorsupportthemovement;ifyouareworkinginIT,youcanhelpIndependentistorganisationsbuildtheirwebsites,can’tyou?IamconfidentthateverycitizenhavearoletoplayinpromotingHongKongIndependence.Theyeachhavetheabilitytocontribute.Manypeople(mistakenly)claim:“oh,revolutionmustbeasuddenoutburst,itmustbedonewithguns,theremustbeconfrontationwiththepolice,orthrowingbricks”,sooneandsoforth.Butrevolutionnotonlyasuddenoutburstofpassion,itisalsoarationalandcalculatedmove.SunYat-senattemptedagrandtotalofelevenrevolutionsinChina.Theeleventhtimefinallyworkedout,andthat,theRevolutionof1911wasjustafluke-theHupehgarrisonwasrotatedintoSzechwan.Still,withoutDrSun,orHuangHsingandtheReviveChinaSociety,withoutthemevangelizingthecauseintheSouthSeas,withoutthemspreadingideasaboutrevolution,therevolutionwouldhaveneveroccurredinthefirstplace.AndnowwhatHongkongersshoulddointheselongandbroodingdays,besideswaitingfortheoncomingrevolution,istospreadthewords.Talktoyourcolleagues,yourparents,yourfriends-tellthemwhyHongKongmustbecomeindependentnow,whyHongKongIndependenceistheonlywayoutforeachandeverycitizen.Thepathtoindependencemaybeperilous,butlooktothosestandingwithyou,lookatthosefamiliarfaces.Instrivingtowardsindependence,youwillnotbealone.InthesenineteenyearsthePan-Democratshavepreachedtheconceptofdemocracyatrallies,buthowmanytrulyunderstandwhatdemocracyis?ThePan-Democratshavetheirinterpretation,thePro-Establishmentshaveanothertwistedandtorturedinterpretation,yetIreckonthattherearetwoconceptscentraltodemocracy:first,thepeople,thatisthenationofHongKong;secondly,thesovereigntyofHongKong.ThePan-DemocratsalwayssaythatevenunderthedominionofChina,wecanstillfightfordemocracy,butwecan’t.TheElectoralAffairsCommissiondeprivedsomanypro-independencecandidatesoftheirrighttoruninanelection,thispreciselyshowsthatunderthegripofChina,youHongkongerswouldabsolutelynotbeabletocalltheshots.Democracyisjustself-governance,self-determination.YetundertheoppressionofChina,itcanneverbeachievedbyHongkongers.Whywehavetodeclareindependence,isbecausewehavetotakebackoursovereignty,beforewecancontrolourdestiny.Manypeoplesaythat“peaceful,rational,non-violent,non-profane”assembliesareuseless,butIdisagree,becausethisisthefirsttimetheideaofHongKongIndependenceisopenlypromotedtoallHongkongersinsuchapublicmannerandinsuchalargescale.Todayisjustaforum,arallyatthisTamarPark,avenuewherewecanexchangeideas.ButIhopethatinthefuture,intheforeseeablefuture,wecanreturntothisplace,notforanotherrally,butforeverycitizentogatheraroundandmakeourownHongKong’sTennisCourtOath<unk>Thankyoueveryone.大家好，我係香港民族黨嘅發言人！咁我上次禮拜五集會講錯咗<unk>集人，咁今次講返<unk>啦，我係發言人，係嘞。咁我叫周浩<unk>，咁今日就有一個咁大型嘅和平集會啦，好多人對和平集會都有一個厭惡，或者係一個質疑啦，咁因為泛民主派過去十九年都係玩爛咗呢一個和平集會。其實suppose和平集會係for積聚呢一個人流啦，然後就等一個爆發點，然後就係有一啲街頭抗爭，或者係一個反政府嘅行動，咁但係泛民主派過往就係喺民怨最<unk><unk>或者最激<unk>嘅時候呢，就同大家講「請沿著呢一個維園啦，<unk>如維園嘅話嘞，請沿著邊一度嘅出口就離開，跟住返屋企」咁樣樣。咁就令到好多好好嘅機會呢，都無機會爆發咁樣樣。咁呀今日喺正式我想講啲嘢之前，我就講下我哋民族黨嘅一啲宣傳品嘅，咁我相信大家嚟嘅時候呢，喺天橋呢，姐金鐘嗰邊姐海<unk>天橋啦，都會收到份文宣嘅，咁上面呢寫住獨立！跟住一個感嘆號。咁究竟呢份嘢講咩呢，咁你有揸喺手你可以宜家舉高啦，姐係去睇下啦，其實入面就係寫住好多香港人或者係好多一般嘅市民都質疑嘅一樣嘢，究竟香港可以點樣獨立，或者，點解香港係有能力成為一個獨立嘅主權國家。佢哋好鍾意講嘅，「喂宜家獨立咁香港冇水飲喎，餓死你班廢青啦」，咁其實唔係嘅，咁，香港其實有好多，姐糧食方面係唔係依賴中國嘅，中國其實係第一大嘅糧食進口國嚟，水方面其實新加坡係用緊海水化淡。咁香港亦都可以<unk>效。咁軍隊方面好多人就話會出解放軍<unk>城，咁，咁九二八都無出解放軍啦，淨係出咗啲警察姐，咁點解佢唔出解放軍係好重要，因為香港呢一撻地方係收埋咗習近平及其親屬同埋呢一個中共高層好多資金、資產喺香港，作為一個資產轉移之用啦，咁樣。咁所以，話解放軍係會出動然後係鎮壓香港，呢個係一個匪<unk>所思啦，我亦都唔相信呢樣嘢會發生。咁軍隊方面好多人就話會出解放軍<unk>城，咁，咁九二八都無出解放軍啦，淨係出咗啲警察姐，咁點解佢唔出解放軍係好重要，因為香港呢一撻地方係收埋咗習近平及其親屬同埋呢一個中共高層好多資金、資產喺香港，作為一個資產轉移之用啦，咁樣。咁所以，話解放軍係會出動然後係鎮壓香港，呢個係一個匪<unk>所思啦，我亦都唔相信呢樣嘢會發生。其實姐隔離嘅解放軍軍營呢，大家唔好以為住著嗰啲，姐<unk><unk><unk>黑呀，好健碩嘅解放軍，其實嚟得香港做<unk>軍嘅呢，全部都係靠關係<unk>通嘅，因為喺香港做<unk>軍呢，就好正嘅，呀，唔使咁辛苦嘅。咁所以解放軍係咪一個香港人應該恐懼嘅對象，或者係香港人係要突破嘅一個心魔，其實唔係。香港點解咁多年都唔能夠係喺呢個叫政治上做突破，係因為香港人其實面對最大嘅心魔就係自己。因為香港人無勇氣係去面對著心裡面嗰份恐懼，覺得<unk>個頭出嚟就會俾人鎮壓，或者係會俾人抄家，呢一份恐懼係嚟自一九八九年六四，老一輩親眼喺電視機又好，或者喺北京嘅現場都好，目<unk>嘅<unk>克<unk>城、解放軍<unk>城，所導致嘅，咁樣樣。咁呀，當然講返啲香港能夠獨立嘅條件啦，正式開始我想講嘅嘢。其實香港人好多苦難嘅，十九年間，啊。一九九七年主權移交之後香港人，由開初話想爭取呢一個政治權利，去到今日，大家為著你哋自己最基本、最基本嘅生存空間，喺度掙扎奮鬥。我哋今日面對嘅，好多香港人面對嘅唔單單係無咗政治權利，或者無最基本嘅自由，而係連你哋最基本嘅生存都受到威<unk>。你哋香港人非常之難，搵一個租所去住啦，甚至係你哋死左之後呢，你哋嘅<unk>位呢，都係一<unk>難求嘅。姐係香港人係連死呀，都係十分之困難嘅。可想而知你話喺香港生存係更加困難嘅一樣嘢。咁究竟係啲乜嘢係令到香港人，喺呢十九年間痛不欲生，由初初只不過係追求呢一個民主自由，去到今日要為自己最基本嘅生存權利同埋空間係去奮鬥，就係因為一九九七年，嘅嗰一個夜晚，香港人、香港唔係選擇咗一個獨立自強嘅道路，而係選擇咗，或者，唔係你哋選擇啦，係有一班所謂嘅知識分子啦，所謂嘅精英啦，係去越<unk>代<unk>，幫你哋咁多香港人選擇咗，幫你哋香港人嘅下一代選擇咗，要所謂回歸中國，佢哋覺得民主回歸，係一個正確嘅事，係民主回歸先至係正確，係大義所在，結果今日嘅香港人，包括我哋呢一代，同埋呢一代以後啦，都要承受呢一種嘅惡果，都要為著自己最基本嘅生存嘅權利同埋自由，係去掙扎。其實我想講就係，香港人其實係絕對有能力，係去自己管治自己嘅。我相信香港人係有咁樣嘅能力，係去創造一個幸福嘅社會，我亦都相信香港人，係有咁樣嘅能力喺國際之間，係能夠出<unk>，係可以同<unk>美先進國家，可以同日本呢啲咁好嘅國家，係鼎足而立。但點解今日嘅香港人要淪落到一個地步，就係話一場立法會嘅選舉，「啊，好擔心入唔入到閘，入到閘，我就要慶祝一番」，會淪落到咁呢？好多人話香港嘅問題，其實香港嘅問題，香港一路都冇問題，所有嘅問題都係中國人嘅問題，因為中國先至係問題嘅所在，中國政府先係問題嘅所在，唔係香港人你哋。香港人虛耗嘅時間其實係太多喇。因為呢十九年間，你哋，係無做過嘢嘅。冇做過嘢嘅意思就係，你哋十九年間都係做著一啲和理非非嘅一個嘅集會，而和理非非嘅集會唔係有問題，個本質係冇問題，但你哋從和理非非嘅集會，從泛民組織嘅集會嗰度，係得唔到任何嘢，你哋亦都冇喺香港淪陷嘅嗰一刻，開始組織反抗嘅力量，呢個先係問題嘅所在。香港人你哋起步得太遲喇，你哋要急起直追。好多人話「嘩，你搞呢個香港獨立，搞港獨，你點樣搞啊」或者話呀，香港人，姐我哋講㗎，「港豬點會理你哋呀，佢哋仲要返工」，我想講嘅係，其實每一個香港人，係推動香港獨立，呢個目標上面，都可以有自己角色，都可以有自己能夠做到嘅嘢，好似呀羊子<unk><unk>咁講，如果你係中產嘅，你咪盡你最基本嘅能力，做呢個叫金主或者後援嘅角色囉；如果你係搞IT嘅，呀，咁你咪去幫呢一個叫做獨派嘅組織，係去做website囉，係咪？我相信香港社會，任何一個人，喺推動香港獨立呢一個議程上面，都有佢嘅角色，都有佢嘅能力，都可以有佢嘅貢獻。好多人話，「啊，革命一定係要嗰當下就要爆發，一定係要揸著槍，然後就係同呢個警察衝突呀，或者係<unk>磚」咁樣。革命，係一場一瞬間嘅情感爆發，更加係一場理性，同埋計算嘅一個鋪排。孫中山佢搞中國嘅革命，佢搞咗足足十一次，第十一次先成功，而第十一次成功嗰一次，辛<unk>革命，佢只不過係符碌，湖北嘅<unk>軍調咗去四<unk>。但係，如果無孫中山，或者黃興一班咁樣嘅興中會，係去不停喺南洋播道呢一個中國革命，或者係做一個思想傳播嘅工作，革命係唔會發生。而香港人，宜家要做嘅，除咗係要慢慢等待嗰場嘅革命嘅爆發之外，更加要係革命爆發呢一段長久、鬱<unk>、屈<unk>難耐嘅一段等待嘅時間去做嘢，做嘅嘢就係思想嘅傳播。係去同你嘅同學講，同你嘅父母講，同你嘅朋友講，點解香港去到呢一刻一定要獨立，點解香港獨立先至係所有香港人嘅出路。香港獨立呢條路可能好<unk>難，但望下你身邊嘅香港人，望下你身邊熟悉嘅面<unk>，喺香港獨立呢一個目標或者進程上面，你會有你嘅支持。十九年間，泛民主派喺和平集會向香港人講好多民主嘅理念，但有幾多人知道民主究竟係啲咩？泛民主派對於民主有一套演<unk>，建制派對民主又有第二種嘅扭曲，但我認為嘅民主，係有兩個概念，第一：民，係指香港民族；第二：主，係指香港嘅主權。泛民主派經常話喺呢個中國嘅殖民統治之下，我哋可以爭取到民主，但我哋唔可以。因為今次選管會剝奪咁多位主張香港獨立候選人嘅參選權利，就正正證明咗喺香港，喺中國嘅殖民統治之下，你哋香港人係絕對唔能夠自己話事。民主就係自己管理自己，自己決定自己嘅命運，但喺中國嘅殖民統治之下，呢一點係香港人永遠都無可能做得到。點解要獨立，就係因為香港人，要掌握自己嘅主權，先至可以掌握到自己嘅命運。今日呢度好多人話和理非非無用，但我唔認同，因為香港獨立呢個理念，係第一次咁公開、咁大型，向全香港人傳播。香港人喺未來嘅路上，我好希望，姐今日呢，就只不過係添馬公園呢度，係做一個演講啦，或者係一個集會啦，大家可以交流啦，我好希望就係，喺不久嘅將來，喺可見嘅將來，大家可以一齊重來呢個地方，並唔係再為咗再一次舉行呢個集會，而係到時我哋全香港嘅人可以聚集喺呢度，發表我哋香港自己嘅網球場宣言！多謝各位。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        predictions = selected_model(batch.text)\n",
    "        max_preds = predictions.argmax(dim=1)\n",
    "        wrong_preds = (max_preds.eq(batch.label) == 0).nonzero()\n",
    "        for wrong_pred in wrong_preds:\n",
    "            wrong_idx = wrong_pred.item()\n",
    "            incorrect_prediction = max_preds[wrong_idx].item()\n",
    "            correct_label = batch.label[wrong_idx].item()\n",
    "            print(\"Predicted \\\"\" + LABEL.vocab.itos[incorrect_prediction] + \"\\\" but should be \\\"\" + LABEL.vocab.itos[correct_label] + \"\\\".\")\n",
    "            text_i = batch.text[:,wrong_idx].tolist()\n",
    "            text_striped_idx = [x for x in text_i if x != TEXT.vocab.stoi['<pad>']]\n",
    "            full_text = list(map(lambda x: TEXT.vocab.itos[x], text_striped_idx))\n",
    "            print(\"Article: \")\n",
    "            print(\"\".join(full_text))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
