{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hong Kongese Language Identifier\n",
    "This notebook contains modifications to make it run with the Hong Kongese language identification dataset. The only difference is that we do not load the English vectors because they will be useless on Hong Kongese.\n",
    "This notebook uses a dataset with 500 each of Hong Kongese and Standard Chinese articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Multi-class Sentiment Analysis\n",
    "\n",
    "In all of the previous notebooks we have performed sentiment analysis on a dataset with only two classes, positive or negative. When we have only two classes our output can be a single scalar, bound between 0 and 1, that indicates what class an example belongs to. When we have more than 2 examples, our output must be a $C$ dimensional vector, where $C$ is the number of classes.\n",
    "\n",
    "In this notebook, we'll be performing classification on a dataset with 6 classes. Note that this dataset isn't actually a sentiment analysis dataset, it's a dataset of questions and the task is to classify what category the question belongs to. However, everything covered in this notebook applies to any dataset with examples that contain an input sequence belonging to one of $C$ classes.\n",
    "\n",
    "Below, we setup the fields, and load the dataset. \n",
    "\n",
    "The first difference is that we do not need to set the `dtype` in the `LABEL` field. When doing a mutli-class problem, PyTorch expects the labels to be numericalized `LongTensor`s. \n",
    "\n",
    "The second different is that we use `TREC` instead of `IMDB` to load the `TREC` dataset. The `fine_grained` argument allows us to use the fine-grained labels (of which there are 50 classes) or not (in which case they'll be 6 classes). You can change this how you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATASET=\"500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tokenizer to simply split at character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    return list(map(str, text.replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from data/language directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenizer)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "fields = {'language': ('label', LABEL), 'text': ('text', TEXT)}\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = 'data/language/' + DATASET,\n",
    "                                        train = 'train.json',\n",
    "                                        validation = 'valid.json',\n",
    "                                        test = 'test.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'en',\n",
       " 'text': ['(',\n",
       "  'T',\n",
       "  'h',\n",
       "  'i',\n",
       "  's',\n",
       "  'a',\n",
       "  'r',\n",
       "  't',\n",
       "  'i',\n",
       "  'c',\n",
       "  'l',\n",
       "  'e',\n",
       "  'c',\n",
       "  'o',\n",
       "  'n',\n",
       "  't',\n",
       "  'a',\n",
       "  'i',\n",
       "  'n',\n",
       "  's',\n",
       "  's',\n",
       "  'p',\n",
       "  'o',\n",
       "  'i',\n",
       "  'l',\n",
       "  'e',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'o',\n",
       "  'v',\n",
       "  'i',\n",
       "  'e',\n",
       "  'L',\n",
       "  'o',\n",
       "  'g',\n",
       "  'a',\n",
       "  'n',\n",
       "  '.',\n",
       "  ')',\n",
       "  '\\xa0',\n",
       "  '\\xa0',\n",
       "  'L',\n",
       "  'o',\n",
       "  'g',\n",
       "  'a',\n",
       "  'n',\n",
       "  ',',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'o',\n",
       "  's',\n",
       "  't',\n",
       "  'r',\n",
       "  'e',\n",
       "  'c',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'X',\n",
       "  '-',\n",
       "  'M',\n",
       "  'e',\n",
       "  'n',\n",
       "  'l',\n",
       "  'i',\n",
       "  'v',\n",
       "  'e',\n",
       "  'a',\n",
       "  'c',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  'f',\n",
       "  'i',\n",
       "  'l',\n",
       "  'm',\n",
       "  's',\n",
       "  'e',\n",
       "  'r',\n",
       "  'i',\n",
       "  'e',\n",
       "  's',\n",
       "  '–',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  's',\n",
       "  'u',\n",
       "  'p',\n",
       "  'p',\n",
       "  'o',\n",
       "  's',\n",
       "  'e',\n",
       "  'd',\n",
       "  'l',\n",
       "  'y',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'l',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  't',\n",
       "  'i',\n",
       "  'm',\n",
       "  'e',\n",
       "  'H',\n",
       "  'u',\n",
       "  'g',\n",
       "  'h',\n",
       "  'J',\n",
       "  'a',\n",
       "  'c',\n",
       "  'k',\n",
       "  'm',\n",
       "  'a',\n",
       "  'n',\n",
       "  'w',\n",
       "  'o',\n",
       "  'u',\n",
       "  'l',\n",
       "  'd',\n",
       "  'b',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'r',\n",
       "  'r',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'W',\n",
       "  'o',\n",
       "  'l',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'i',\n",
       "  'n',\n",
       "  'e',\n",
       "  '–',\n",
       "  't',\n",
       "  'o',\n",
       "  'l',\n",
       "  'd',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  't',\n",
       "  'a',\n",
       "  'l',\n",
       "  'e',\n",
       "  'o',\n",
       "  'f',\n",
       "  'a',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'o',\n",
       "  'c',\n",
       "  'i',\n",
       "  'd',\n",
       "  'e',\n",
       "  ',',\n",
       "  'a',\n",
       "  'g',\n",
       "  'o',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'm',\n",
       "  'o',\n",
       "  'n',\n",
       "  'o',\n",
       "  'p',\n",
       "  'o',\n",
       "  'l',\n",
       "  'y',\n",
       "  'o',\n",
       "  'f',\n",
       "  's',\n",
       "  'p',\n",
       "  'e',\n",
       "  'c',\n",
       "  'i',\n",
       "  'a',\n",
       "  'l',\n",
       "  'p',\n",
       "  'o',\n",
       "  'w',\n",
       "  'e',\n",
       "  'r',\n",
       "  's',\n",
       "  ',',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'l',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  'f',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  'd',\n",
       "  ',',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'g',\n",
       "  'l',\n",
       "  'i',\n",
       "  'm',\n",
       "  'm',\n",
       "  'e',\n",
       "  'r',\n",
       "  'o',\n",
       "  'f',\n",
       "  'h',\n",
       "  'o',\n",
       "  'p',\n",
       "  'e',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  'o',\n",
       "  'f',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'p',\n",
       "  'o',\n",
       "  'p',\n",
       "  'u',\n",
       "  'l',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  '.',\n",
       "  'W',\n",
       "  'h',\n",
       "  'i',\n",
       "  'l',\n",
       "  'e',\n",
       "  'n',\n",
       "  'o',\n",
       "  'c',\n",
       "  'o',\n",
       "  'm',\n",
       "  'p',\n",
       "  'a',\n",
       "  'r',\n",
       "  'i',\n",
       "  's',\n",
       "  'o',\n",
       "  'n',\n",
       "  'c',\n",
       "  'a',\n",
       "  'n',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'd',\n",
       "  'o',\n",
       "  'j',\n",
       "  'u',\n",
       "  's',\n",
       "  't',\n",
       "  'i',\n",
       "  'c',\n",
       "  'e',\n",
       "  't',\n",
       "  'o',\n",
       "  'a',\n",
       "  's',\n",
       "  'i',\n",
       "  't',\n",
       "  'u',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  ',',\n",
       "  'L',\n",
       "  'o',\n",
       "  'g',\n",
       "  'a',\n",
       "  'n',\n",
       "  '’',\n",
       "  's',\n",
       "  'p',\n",
       "  'l',\n",
       "  'o',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  's',\n",
       "  'a',\n",
       "  'n',\n",
       "  'o',\n",
       "  'm',\n",
       "  'i',\n",
       "  'n',\n",
       "  'o',\n",
       "  'u',\n",
       "  's',\n",
       "  'f',\n",
       "  'a',\n",
       "  'm',\n",
       "  'i',\n",
       "  'l',\n",
       "  'i',\n",
       "  'a',\n",
       "  'r',\n",
       "  'i',\n",
       "  't',\n",
       "  'y',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  'H',\n",
       "  'o',\n",
       "  'n',\n",
       "  'g',\n",
       "  'K',\n",
       "  'o',\n",
       "  'n',\n",
       "  'g',\n",
       "  '.',\n",
       "  '\\xa0',\n",
       "  'T',\n",
       "  'h',\n",
       "  'e',\n",
       "  'G',\n",
       "  'e',\n",
       "  'n',\n",
       "  'o',\n",
       "  'c',\n",
       "  'i',\n",
       "  'd',\n",
       "  'e',\n",
       "  '\\xa0',\n",
       "  'I',\n",
       "  't',\n",
       "  'i',\n",
       "  's',\n",
       "  '2',\n",
       "  '0',\n",
       "  '2',\n",
       "  '9',\n",
       "  ',',\n",
       "  'a',\n",
       "  'f',\n",
       "  'u',\n",
       "  't',\n",
       "  'u',\n",
       "  'r',\n",
       "  'e',\n",
       "  'w',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  't',\n",
       "  'i',\n",
       "  'g',\n",
       "  'e',\n",
       "  'r',\n",
       "  'h',\n",
       "  'a',\n",
       "  's',\n",
       "  'b',\n",
       "  'e',\n",
       "  'c',\n",
       "  'o',\n",
       "  'm',\n",
       "  'e',\n",
       "  'e',\n",
       "  'x',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'c',\n",
       "  't',\n",
       "  'd',\n",
       "  'u',\n",
       "  'e',\n",
       "  't',\n",
       "  'o',\n",
       "  'm',\n",
       "  'a',\n",
       "  's',\n",
       "  's',\n",
       "  'p',\n",
       "  'o',\n",
       "  'a',\n",
       "  'c',\n",
       "  'h',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ',',\n",
       "  'a',\n",
       "  'm',\n",
       "  'a',\n",
       "  'r',\n",
       "  'k',\n",
       "  'e',\n",
       "  'r',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'a',\n",
       "  'u',\n",
       "  'd',\n",
       "  'i',\n",
       "  'e',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  'w',\n",
       "  'o',\n",
       "  'u',\n",
       "  'l',\n",
       "  'd',\n",
       "  'h',\n",
       "  'a',\n",
       "  'v',\n",
       "  'e',\n",
       "  'm',\n",
       "  'i',\n",
       "  's',\n",
       "  's',\n",
       "  'e',\n",
       "  'd',\n",
       "  'b',\n",
       "  'u',\n",
       "  't',\n",
       "  's',\n",
       "  'i',\n",
       "  'g',\n",
       "  'n',\n",
       "  'i',\n",
       "  'f',\n",
       "  'i',\n",
       "  'c',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  'r',\n",
       "  'y',\n",
       "  '’',\n",
       "  's',\n",
       "  'm',\n",
       "  'e',\n",
       "  'a',\n",
       "  'n',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  '.',\n",
       "  'F',\n",
       "  'o',\n",
       "  'r',\n",
       "  '2',\n",
       "  '5',\n",
       "  'y',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'U',\n",
       "  'n',\n",
       "  'i',\n",
       "  't',\n",
       "  'e',\n",
       "  'd',\n",
       "  'S',\n",
       "  't',\n",
       "  'a',\n",
       "  't',\n",
       "  'e',\n",
       "  's',\n",
       "  'g',\n",
       "  'o',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  'v',\n",
       "  'e',\n",
       "  'e',\n",
       "  'n',\n",
       "  'a',\n",
       "  'c',\n",
       "  't',\n",
       "  'e',\n",
       "  'd',\n",
       "  'a',\n",
       "  'b',\n",
       "  'l',\n",
       "  'o',\n",
       "  'o',\n",
       "  'd',\n",
       "  'l',\n",
       "  'e',\n",
       "  's',\n",
       "  's',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'o',\n",
       "  'c',\n",
       "  'i',\n",
       "  'd',\n",
       "  'a',\n",
       "  'l',\n",
       "  'p',\n",
       "  'o',\n",
       "  'l',\n",
       "  'i',\n",
       "  'c',\n",
       "  'y',\n",
       "  'i',\n",
       "  'n',\n",
       "  'c',\n",
       "  'u',\n",
       "  'r',\n",
       "  't',\n",
       "  'a',\n",
       "  'i',\n",
       "  'l',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'g',\n",
       "  'r',\n",
       "  'o',\n",
       "  'w',\n",
       "  't',\n",
       "  'h',\n",
       "  'o',\n",
       "  'f',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  'd',\n",
       "  ',',\n",
       "  's',\n",
       "  'o',\n",
       "  'e',\n",
       "  'f',\n",
       "  'f',\n",
       "  'e',\n",
       "  'c',\n",
       "  't',\n",
       "  'i',\n",
       "  'v',\n",
       "  'e',\n",
       "  'i',\n",
       "  's',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'o',\n",
       "  'p',\n",
       "  'e',\n",
       "  'r',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  'n',\n",
       "  'o',\n",
       "  'n',\n",
       "  'e',\n",
       "  'w',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  's',\n",
       "  'h',\n",
       "  'a',\n",
       "  'd',\n",
       "  'b',\n",
       "  'e',\n",
       "  'e',\n",
       "  'n',\n",
       "  'b',\n",
       "  'o',\n",
       "  'r',\n",
       "  'n',\n",
       "  'd',\n",
       "  'u',\n",
       "  'r',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  't',\n",
       "  'h',\n",
       "  'i',\n",
       "  's',\n",
       "  'w',\n",
       "  'h',\n",
       "  'o',\n",
       "  'l',\n",
       "  'e',\n",
       "  't',\n",
       "  'i',\n",
       "  'm',\n",
       "  'e',\n",
       "  '.',\n",
       "  'W',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  's',\n",
       "  'e',\n",
       "  'e',\n",
       "  'm',\n",
       "  'e',\n",
       "  'd',\n",
       "  't',\n",
       "  'o',\n",
       "  'h',\n",
       "  'a',\n",
       "  'v',\n",
       "  'e',\n",
       "  'h',\n",
       "  'a',\n",
       "  'p',\n",
       "  'p',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'd',\n",
       "  'w',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'g',\n",
       "  'o',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'n',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  'd',\n",
       "  'i',\n",
       "  'n',\n",
       "  'f',\n",
       "  'u',\n",
       "  's',\n",
       "  'e',\n",
       "  'd',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'n',\n",
       "  'u',\n",
       "  'l',\n",
       "  'l',\n",
       "  'i',\n",
       "  'f',\n",
       "  'y',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'e',\n",
       "  'l',\n",
       "  'e',\n",
       "  'm',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  's',\n",
       "  'i',\n",
       "  'n',\n",
       "  't',\n",
       "  'o',\n",
       "  'f',\n",
       "  'o',\n",
       "  'o',\n",
       "  'd',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'w',\n",
       "  'a',\n",
       "  't',\n",
       "  'e',\n",
       "  'r',\n",
       "  ',',\n",
       "  's',\n",
       "  'l',\n",
       "  'o',\n",
       "  'w',\n",
       "  'l',\n",
       "  'y',\n",
       "  'd',\n",
       "  'e',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'e',\n",
       "  'x',\n",
       "  'i',\n",
       "  's',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  's',\n",
       "  '’',\n",
       "  'p',\n",
       "  'o',\n",
       "  'w',\n",
       "  'e',\n",
       "  'r',\n",
       "  's',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'p',\n",
       "  'r',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'n',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'r',\n",
       "  'a',\n",
       "  'n',\n",
       "  's',\n",
       "  'm',\n",
       "  'i',\n",
       "  't',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  't',\n",
       "  'o',\n",
       "  'e',\n",
       "  'a',\n",
       "  'c',\n",
       "  'h',\n",
       "  'n',\n",
       "  'e',\n",
       "  'w',\n",
       "  'g',\n",
       "  'e',\n",
       "  'n',\n",
       "  'e',\n",
       "  'r',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n',\n",
       "  '.',\n",
       "  'I',\n",
       "  'n',\n",
       "  'L',\n",
       "  'o',\n",
       "  'g',\n",
       "  'a',\n",
       "  'n',\n",
       "  ',',\n",
       "  'a',\n",
       "  'l',\n",
       "  'l',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  'r',\n",
       "  'e',\n",
       "  'm',\n",
       "  'a',\n",
       "  'i',\n",
       "  'n',\n",
       "  's',\n",
       "  'o',\n",
       "  'f',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  't',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  'd',\n",
       "  'w',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'W',\n",
       "  'o',\n",
       "  'l',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  'i',\n",
       "  'n',\n",
       "  'e',\n",
       "  ',',\n",
       "  'C',\n",
       "  'h',\n",
       "  'a',\n",
       "  'r',\n",
       "  'l',\n",
       "  'e',\n",
       "  's',\n",
       "  'X',\n",
       "  'a',\n",
       "  'v',\n",
       "  'i',\n",
       "  'e',\n",
       "  'r',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'C',\n",
       "  'a',\n",
       "  'l',\n",
       "  'i',\n",
       "  'b',\n",
       "  'a',\n",
       "  'n',\n",
       "  ';',\n",
       "  'b',\n",
       "  'u',\n",
       "  't',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  'r',\n",
       "  'y',\n",
       "  'u',\n",
       "  ...]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build the vocabulary. As this dataset is small (only ~3800 training examples) it also has a very small vocabulary (~7500 unique tokens), this means we do not need to set a `max_size` on the vocabulary as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can check the labels.\n",
    "\n",
    "The 6 labels (for the non-fine-grained case) correspond to the 6 types of questions in the dataset:\n",
    "- `HUM` for questions about humans\n",
    "- `ENTY` for questions about entities\n",
    "- `DESC` for questions asking you for a description \n",
    "- `NUM` for questions where the answer is numerical\n",
    "- `LOC` for questions where the answer is a location\n",
    "- `ABBR` for questions asking about abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x116f12268>, {'hky': 0, 'zh': 1, 'en': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we set up the iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the CNN model from the previous notebook, however any of the models covered in these tutorials will work on this dataset. The only difference is now the `output_dim` will be $C$ instead of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model, making sure to set `OUTPUT_DIM` to $C$. We can get $C$ easily by using the size of the `LABEL` vocab, much like we used the length of the `TEXT` vocab to get the size of the vocabulary of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another different to the previous notebooks is our loss function (aka criterion). Before we used `BCEWithLogitsLoss`, however now we use `CrossEntropyLoss`. Without going into too much detail, `CrossEntropyLoss` performs a *softmax* function over our model outputs and the loss is given by the *cross entropy* between that and the label.\n",
    "\n",
    "Generally:\n",
    "- `CrossEntropyLoss` is used when our examples exclusively belong to one of $C$ classes\n",
    "- `BCEWithLogitsLoss` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over 0.5 then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class. \n",
    "\n",
    "For example, in our labels we have: 'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5. If the output of our model was something like: **[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]** this means that the model strongly believes the example belongs to class 0, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
    "\n",
    "We calculate the accuracy by performing an `argmax` to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum()/torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is similar to before, without the need to `squeeze` the model predictions as `CrossEntropyLoss` expects the input to be **[batch size, n classes]** and the label to be **[batch size]**.\n",
    "\n",
    "The label needs to be a `LongTensor`, which it is by default as we did not set the `dtype` to a `FloatTensor` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is, again, similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/keras/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.849 | Train Acc: 60.14% | Val. Loss: 0.269 | Val. Acc: 97.27% | Saved: True\n",
      "| Epoch: 02 | Train Loss: 0.385 | Train Acc: 84.93% | Val. Loss: 0.136 | Val. Acc: 94.92% | Saved: True\n",
      "| Epoch: 03 | Train Loss: 0.239 | Train Acc: 90.88% | Val. Loss: 0.085 | Val. Acc: 99.22% | Saved: True\n",
      "| Epoch: 04 | Train Loss: 0.155 | Train Acc: 94.22% | Val. Loss: 0.074 | Val. Acc: 98.83% | Saved: True\n",
      "| Epoch: 05 | Train Loss: 0.125 | Train Acc: 94.99% | Val. Loss: 0.066 | Val. Acc: 98.83% | Saved: True\n",
      "| Epoch: 06 | Train Loss: 0.099 | Train Acc: 97.02% | Val. Loss: 0.063 | Val. Acc: 98.83% | Saved: True\n",
      "| Epoch: 07 | Train Loss: 0.079 | Train Acc: 96.93% | Val. Loss: 0.062 | Val. Acc: 98.83% | Saved: True\n",
      "| Epoch: 08 | Train Loss: 0.093 | Train Acc: 96.42% | Val. Loss: 0.062 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 09 | Train Loss: 0.083 | Train Acc: 97.15% | Val. Loss: 0.065 | Val. Acc: 98.44% | Saved: False\n",
      "| Epoch: 10 | Train Loss: 0.075 | Train Acc: 97.06% | Val. Loss: 0.055 | Val. Acc: 99.22% | Saved: True\n",
      "| Epoch: 11 | Train Loss: 0.054 | Train Acc: 98.40% | Val. Loss: 0.058 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 12 | Train Loss: 0.072 | Train Acc: 97.24% | Val. Loss: 0.063 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 13 | Train Loss: 0.061 | Train Acc: 97.70% | Val. Loss: 0.064 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 14 | Train Loss: 0.040 | Train Acc: 98.86% | Val. Loss: 0.060 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 15 | Train Loss: 0.055 | Train Acc: 98.53% | Val. Loss: 0.063 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 16 | Train Loss: 0.040 | Train Acc: 98.35% | Val. Loss: 0.061 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 17 | Train Loss: 0.039 | Train Acc: 98.90% | Val. Loss: 0.056 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 18 | Train Loss: 0.053 | Train Acc: 98.07% | Val. Loss: 0.061 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 19 | Train Loss: 0.027 | Train Acc: 99.26% | Val. Loss: 0.069 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 20 | Train Loss: 0.029 | Train Acc: 99.17% | Val. Loss: 0.058 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 21 | Train Loss: 0.027 | Train Acc: 99.36% | Val. Loss: 0.092 | Val. Acc: 98.05% | Saved: False\n",
      "| Epoch: 22 | Train Loss: 0.030 | Train Acc: 98.95% | Val. Loss: 0.061 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 23 | Train Loss: 0.027 | Train Acc: 99.26% | Val. Loss: 0.058 | Val. Acc: 98.83% | Saved: False\n",
      "| Epoch: 24 | Train Loss: 0.028 | Train Acc: 99.26% | Val. Loss: 0.061 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 25 | Train Loss: 0.025 | Train Acc: 99.08% | Val. Loss: 0.084 | Val. Acc: 98.44% | Saved: False\n",
      "| Epoch: 26 | Train Loss: 0.019 | Train Acc: 99.54% | Val. Loss: 0.066 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 27 | Train Loss: 0.029 | Train Acc: 99.41% | Val. Loss: 0.058 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 28 | Train Loss: 0.034 | Train Acc: 98.95% | Val. Loss: 0.077 | Val. Acc: 98.44% | Saved: False\n",
      "| Epoch: 29 | Train Loss: 0.016 | Train Acc: 99.72% | Val. Loss: 0.058 | Val. Acc: 99.22% | Saved: False\n",
      "| Epoch: 30 | Train Loss: 0.027 | Train Acc: 98.71% | Val. Loss: 0.068 | Val. Acc: 98.44% | Saved: False\n",
      "CPU times: user 3h 6min 15s, sys: 16min 3s, total: 3h 22min 19s\n",
      "Wall time: 1h 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "N_EPOCHS = 30\n",
    "\n",
    "lowest_valid_loss = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    # save the model with the lowest validation loss for use later\n",
    "    saved = False\n",
    "    if valid_loss < lowest_valid_loss:\n",
    "        lowest_valid_loss = valid_loss\n",
    "        with open(\"./models/language-identifier-\" + DATASET + \"-best.pt\", 'wb') as fb:\n",
    "            saved = True\n",
    "            torch.save(model, fb)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% | Saved: {saved}')\n",
    "    \n",
    "with open(\"./models/language-identifier-\" + DATASET + \"-final.pt\", 'wb') as ff:\n",
    "    torch.save(model, ff)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-fine-grained case, we should get an accuracy of around 90%. For the fine-grained case, we should get around 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.219 | Test Acc: 95.37% |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning tend to overfit the training data if it ran for too many epochs. We'll compare with the best model we've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/language-identifier-\" + DATASET + \"-best.pt\", 'rb') as fbl:\n",
    "    best_model = torch.load(fbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.223 | Test Acc: 94.59% |\n"
     ]
    }
   ],
   "source": [
    "best_model_test_loss, best_model_test_acc = evaluate(best_model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {best_model_test_loss:.3f} | Test Acc: {best_model_test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model with the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use final model.\n"
     ]
    }
   ],
   "source": [
    "if test_loss > best_model_test_loss:\n",
    "    print(\"Will use best_model.\")\n",
    "    selected_model = best_model\n",
    "else:\n",
    "    print(\"Will use final model.\")\n",
    "    selected_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we made a function to predict sentiment for any given sentences, we can now make a function that will predict the class of question given.\n",
    "\n",
    "The only difference here is that instead of using a sigmoid function to squash the input between 0 and 1, we use the `argmax` to get the highest predicted class index. We then use this index with the label vocab to get the human readable label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence, trained_model, min_len=4):\n",
    "    tokenized = tokenizer(sentence)\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = trained_model(tensor)\n",
    "    print(preds)\n",
    "    max_preds = preds.argmax(dim=1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try it out on a few different questions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0592,  6.1842, -3.9069]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 1 = zh\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"特朗普上周四（7日）曾表示，在3月1日達成貿易協議的最後期限前，他不會與中國國家主席習近平會晤。\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2779, -1.1500, -3.9987]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 0 = hky\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"喺未有互聯網之前，你老母叫你做人唔好太高眼角，正正常常嘅男人嫁出去就算。\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5634, -0.9845,  2.0486]], grad_fn=<AddmmBackward>)\n",
      "Predicted class is: 2 = en\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_sentiment(\"I need to get some food.\", selected_model)\n",
    "print(f'Predicted class is: {pred_class} = {LABEL.vocab.itos[pred_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_predictions(prelist, trained_model, min_len=4):\n",
    "    min_len = 4\n",
    "    predict_hky = {}\n",
    "    predict_zh = {}\n",
    "    predict_en = {}\n",
    "    for token in prelist:\n",
    "        tokenized = tokenizer(token)\n",
    "        tokenized_len = len(tokenized)\n",
    "        if tokenized_len < min_len:\n",
    "            tokenized += ['<pad>'] * (min_len - tokenized_len)\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        preds = trained_model(tensor)\n",
    "        max_preds = preds.argmax(dim=1)\n",
    "        if LABEL.vocab.itos[max_preds.item()] == 'hky':\n",
    "            predict_hky[token] = preds.data[0][max_preds.item()].item()\n",
    "        elif LABEL.vocab.itos[max_preds.item()] == 'zh':\n",
    "            predict_zh[token] = preds.data[0][max_preds.item()].item()\n",
    "        else:\n",
    "            predict_en[token] = preds.data[0][max_preds.item()].item()\n",
    "    return predict_hky, predict_zh, predict_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hky, predict_zh, predict_en = range_predictions(TEXT.vocab.itos, selected_model, min_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('嘅', 2.3741226196289062)\n",
      "('唔', 2.373934030532837)\n",
      "('咗', 2.059317111968994)\n",
      "('？', 1.960614562034607)\n",
      "('係', 1.9603140354156494)\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(predict_hky.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('的', 2.023282766342163)\n",
      "('他', 1.1343632936477661)\n",
      "('坡', 1.0784488916397095)\n",
      "('被', 1.0438194274902344)\n",
      "('恭', 1.0360703468322754)\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(predict_zh.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<pad>', 1.1476426124572754)\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(predict_en.items(), key=lambda kv: kv[1])\n",
    "sorted_by_value.reverse()\n",
    "for i in range(5):\n",
    "    if i < len(sorted_by_value):\n",
    "        print(sorted_by_value[i])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what kind of articles are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "已宣布參選下屆特首的前政務司司長林鄭月娥今早於商台節目《在晴朗的一天出發》中，被問到就基本法廿三條立法和政改重啟的立場，林鄭指就廿三條立法是憲制責任，而大家越來越覺得國家安全重要，恐怖活動又無日無之於其他地方發生，認為要考慮環境因素和條件是否可以讓下一屆政府推行這「兩件重要的事」。林鄭指，任何施政也要看環境因素，不能勉強，要權衡輕重，「喺咩時候就係做咩政策」。她舉例指，今屆政府是直到2013年底才啟動政改，而非一上任就做政改工作，是因為這屆政府首先答應推行扶貧安老助養、改善民生的政策，讓社會氣氛好一點才做政改。她指，以後的政府也要用這種思維，「喺咩時候可以做咩嘢」。而現時廿三條立法和政改重啟兩個議題也富爭議性，壁壘分明，所以下屆特首要「諗幾時做」，但「兩個（議題）都要做」。面對現屆立法會　推行有困難廣告她指，廿三條立法是憲制責任，現在大家越來越覺得國家安全很重要，「呢啲恐怖活動真係無日無之喺其他地方發生緊」，而政改是回應很多市民追求民主發展和一人一票選特首，「所以你問我，就係要睇吓個環境因素、條件，是否可以讓我們喺下一屆推行呢兩件重要的事」。被問到她對大勢評估，下一屆政府著手推行這兩個議題是否比較困難，林鄭直言是很困難，因為政改和廿三條也要由現屆立法會處理，所以現時立法會的組成能否給予政府信心，能獲得足夠支持推行該政策，是一個無可迴避的問題，她指這也是要考慮的因素。廣告手到拿來膽更大？　林鄭：選舉令人謙卑但被問到大方向是否「休養生息」，林鄭指「休養生息」有很多解讀，就如「咩都唔做」，但她認為「香港點可咩都唔做」，因為在經濟面對很多競爭，所以經濟上就不可休養生息。聽眾陳先生問林鄭，若「手到拿來」會否「膽更大」？林鄭回應指，「有一個說法，選舉係令人謙卑，咁我而家都進入呢個狀態」，又指若她有做得不好地方，歡迎市民提出，她也會自我反省，「如果我喺為官呢30多年係有啲地方做得唔好，咁我會趁這機會認識同埋可以改善」。至於會否追究現任特首梁振英UGL事件，林鄭則指會按法治和現行制度處理。對於若由她帶領的政府下，社會撕裂會細一點，林鄭指她有這期望，「如果唔係我都唔會拋個身出嚟，去做一件相當艱鉅的事」，又指特首不只是一份工，要全情投入、步步為營。\n",
      "\n",
      "Predicted \"hky\" but should be \"zh\".\n",
      "Article: \n",
      "再不想上堂，有些堂，點都要上，例如finalyearproject。因為，唔上，就無學分，無學分，就畢唔到業，畢唔到業，就好煩。所以，我跟朱千雪、小清及一名陌生男子組成的小隊，每個星期點都要見一次面。原則上，個finalyearproject的玩法，是四名同學不斷開會，每個星期匯報一下進展。但以我當時的狀態，根本完全無心情跟任何人商量任何題目，所以，一開局，已經有共識，分工合作。我寫晒成份功課，個陌生男負責做powerpoint之類的工具，小清與朱千雪則負責present。我問他們，信任我嗎？認識我的兩個都說相信，餘下的陌生男知道唔使自己做，求之不得。就此下決定。一直相安無事，直到接近埋門一腳，朱千雪終於忍唔住，向我發炮。不是為了份功課，是為了我的態度。可能我一直當她是女神般看待，即使我有女朋友，Amber在我身邊也好，我都會交出一份敬意。到Grace出現了，我的情緒被撕裂得四分五裂，都忘記了怎樣可以不黑面。朱千雪了解我的狀況，明顯不滿：「你咁樣做，對Amber真係好唔公平。」「你都試過一腳踏兩船，你有乜資格話我呀？」我竟然發佢脾氣。「但係我無唔尊重人，無試過專登喺一個面前帶另一個出來走來走去，又無試過光明正大咁未搞掂舊一個就拖住新一個。」「我邊有拖呀？」「有乜分別呀？」朱千雪好少會咁大脾氣，我都唔明佢嬲乜，都唔關佢事。廣告結果，個finalyearproject，每次開會，都有兩個好黑面的怪人。兩年多前，我還在狂追朱千雪；估都估唔到有朝一日會好似仇人咁互啤。餘下兩個成員只好盡力搞氣氛，小清好明顯不是呢方面的高手，陌生男更差。情況非常僵硬。總算勉強頂到最後，我交出一份拎A-的功課，算對得住大家有餘啦。朱千雪仍然沒有高興起來。鄭伊健與梁詠琪已不再是封面人物，但形象大受打擊。始終是香港人，明星偶像要神聖的，最好不結婚不拍拖最好不做愛，難怪郭富城曾經話自己係處男，女星又個個爭住認係教徒。我當年當然是力排眾議支持鄭伊健與梁詠琪的少數啦。尤其梁詠琪，她還未正式入行，只是模特兒，我曾經在信和買了兩張明星照，回學校呃朋友，話俾人聽這是我的女朋友，又真係會有人信。這是發生在李嘉慧事件之前的趣事。對梁詠琪，我從來有份偏好。見到她被圍剿，於心不忍。可惜我自身也難保，即使算做排解了Amber與Grace的困局，但名聲已敗壞，已經不再是那個人見人愛的大組長。廣告最慘是連朱千雪都好似放棄了我。當finalyearproject完結後，大家的關係便告終了。這三年間的感情，脆弱得有如在冰箱中取出一條冰冰雪條，不用幾分鐘，便消失得無影無踪。樓花也完全不見踪影，她發生了什麼事，我毫不知情。最後，我剩下的朋友，就只有小清一個。如果要為這三年大學生涯作個總結，都可以用一事無成來形容。朋友，勉強算做有一個，但小清是小清，我完全估計到她很快會有自己的家庭，然後安心做一個好太太好媽媽，給朋友，尤其像我這類朋友的時間，一年都未必有一日。女朋友，勉強算做有兩個，但大家都在辛苦經營，完全望不見希望。成績，差到只能說勉強畢到業。不過，這個最無所謂，如果三年後，仍然要拿我的學業成績來自吹自擂，我會覺得悲哀。而，很多政客，在畢業三十年後，還把自己在中學做過班長的事當作人生最重要的一個章節來宣傳。唉⋯⋯隔了一排沒有找Grace，不知是否因為朱千雪的態度令我有點難受，我又忍不住去找Grace傾訴。根本完全在將個關係愈搞愈複雜。「Grace吖，唔該。」「我係呀。」「你點呀？」「冇乜嘢。在家，悶。」「我來找你，好不好？」「你又話唔再見我？」「我掛住你，想搵你傾下偈啫。」「你係咪有事呀？」「沒什麼事，只是，就快畢業了，你會跟我影畢業相嗎？」「你夠膽同我一齊影咩？」「唔怕啦，還怕什麼，我都走了。我想至少要同你影一張吧。」「咁你使唔使我送個公仔俾你？你鍾意布甸狗？」「唔好啦，咁核突，都唔知邊個發明要帶個公仔去影畢業相。一定係啲賣公仔嘅人。」「你一定係有啲事發生咗嘅，話我知乜事啦，係咪Amber呀？」「唔係，不過，朱千雪無啦啦嬲咗我啫。」「哦，原來係你個女神嬲咗你。」「女乜鬼神喎，成幾年前的事啦。」「十幾年後，幾十年後，你都會照樣當佢女神㗎啦。」「我有冇咁長情呀？」「你唔係長情，係唔甘心。未得到手就係未得到手，未得到手就點會依依不捨，係咪吖？」我唔識答，我想誠實地去答一聲係，但今日的張國強已經無之前蠢得咁交關，喺適當時候，會懂得沉默。何況，如果根據Grace的說法，我更加放唔低，更加會當佢女神嘅，應該會係李嘉慧。咁，點講得出口呀？最終，我都係搵了Grace，傾起畢業後的前途。「其實，我而家已經可以搵定工啦，finalyearproject做完，我一個星期只係返一日學就夠，搵份長工都有可能得。」「好吖，反正你都係鍾意做嘢多過讀書。咁，你諗住搵份乜工呀？」「雜誌囉，做住先，最好就入到壹仔啦，好似最勁咁。」「《壹週刊》？」「係呀？你話好唔好呀？」「唓，做住先之嘛，佢肯請，咪得囉。」「咁又係，咁等我去火車站拎份《Recruit》先。」做住先，卻變成做做做做做住先。 作者facebook\n",
      "\n",
      "Predicted \"zh\" but should be \"en\".\n",
      "Article: \n",
      "（中譯版本在下）ThecurrentRioOlympicshassadlybeenbrandedasoneoftheugliestinrecentmemoriesasfarastheOlympicspiritisconcerned,orthelackofit.We’vesofarwitnessedmorethanourfairshareoftheunsightlysideofthisglobalsportingevent.BeforetheGamesevenstartedtherewerealreadyrisinganti-RussiansentimentsduetodopingscandalsandthenChineseswimmerSunYangtoppedit,notbybeingcalleda“drugcheat”,buttheensuingretaliatorywarofwordshelaunchedagainsthisaccuserthereafter,whichwasutterlyshameful.Fortunately,theGames’mostcelebratedmedallistMichaelPhelpssavedtheGames’PRimagefromspirallingoutofcontrolwithhisgraciousacknowledgementofhisdefeatinthe100-metrebutterflytoyoungfirst-timeOlympianJosephSchooling,whobecameafanofPhelps’aftermeetinghimeightyearsago.WithsuchadramaticturnofeventsunfoldingattheRioGames,Icouldn’tpossiblystandonthesidelinesandleavetheGamessosoon.Therefore,IdecidedtodedicateanothercolumntotheRioGames.Afterall,ittakesplaceonlyonceeveryfouryears.Whynot?Firstoff,“savetheday”isausefulonetousetodescribesomeonewhoturnsabadsituationaroundandproducesgoodresultslikePhelpshimself.Anothersimilarphraseis“snatchvictoryfromthejawsofdefeat”.Translation:toturndefeatintovictory;simpleenough.OthersportingtermsthathavebeenadaptedtodailyEnglishuseis“jumpthegun”;thetermwasoriginallyusedintrackandfieldeventswhenacompetitorwouldrunbeforethestartingpistolwasshottosignifythebeginningoftherace.Nowadays,it’scommonlyusedwhensomeonedoessomethingtoosoon,usuallyimpulsively.Oneotherusefulphraseis“blowthecompetitionaway”meaningtocompletelyandutterlybeatyouroppositionwithoutachanceofmakingacomeback.Prettyeasytopicture,Iguess.Okayafewmoreinterestingsportingphrasesandseeifyoucanguesswhatsportstheyoriginatedfrom.“Skatingonthinice”?Tooeasy?Itsmeaningisalsoprettyliteral,whichmeansdoingsomethingandputtingoneselfinaverydangerousposition.Howabout“ashotinthedark”?Ifoneistoldnottodosomethingbecauseit’slikeashotinthedark,itmeansthepersonisdoingitwithoutsufficientinformationorknowledge,whichisequivalenttofiringashotinthedark.Okay,whataboutthisone,“hitbelowthebelt”?Whatsportcoulditbe?Yes,yougotit,it’sboxing.Ifyouhitbelowthebeltmeanstodosomethingnottotallylegalorappropriate,likepunchingyouropponentbelowthebeltduringaboxingmatch.Welldoneguys,you’vewonthisonehandsdown,butbeforeIgo,Iwillchallengeyoursportingspirit.Forthenextfewdays,useasmanysportingphrasesaspossibleandtrytoimpressasmanyfriendsandcolleaguesaspossible.Ifyouhavebeenpayingattentiontomycolumnsandpractising,thenIthinkwemaybeontoawinner.Gomakemeproud,champ!Morenextweek.今屆里約奧運是近年最不堪入目的一次，令人心痛之處是，其奧運精神竟至蕩然無存。到目前為此，我們目睹了太多這件體壇盛事不太光采的一面。拜俄羅斯選手服用興奮劑的醜聞所賜，早在比賽開始前，國際間已醞釀起一股反俄情緒。孫楊繼而「再下一城」，但並非其禁藥風波所致，而是他對指控者具報復性、極為丟臉的回話。幸好，比賽最著名的得獎者菲比斯，對曾於八年前見面、如今打敗他的小粉絲──首登奧運即在100米蝶泳奪金的新加坡泳將史高寧（JosephSchooling）的親切表現，成功扭轉奧運近乎失控的公關形象。隨著這戲劇性轉變在里約奧運上演，我再不可能只站在場邊，這麼快就離身離去！所以，我決定再次把專欄獻給奧運，畢竟它也只是四年一度，何樂而不為？首先，「savetheday」形容像菲比斯般將劣局扭轉形勢、把爛攤子收拾妥當的行為。另一個意思相近的片語是「snatchvictoryfromthejawsofdefeat」，解作反敗為勝，夠簡單吧？「Jumpthegun」亦是一個在日常英語廣泛使用的運動術語，它本來指田徑項目中的「偷步」行為，但今時今日，我們可用來形容別人過早或太衝動地做事。另一有用的片語是「blowthecompetitionaway」，代表完完全全、徹底地打敗對手，對方連一絲還擊的機會沒有，從字面就很易想像。「Ashotinthedark」又如何？如果某人被叫不去做某事，因為有如「ashotinthedark」，意思就是他因知識、資料不足，舉動有如在黑暗中開槍。「Hitbelowthebelt」又與哪種運動有關呢？對，就是拳擊。如果選手向腰帶下方的位置出拳，代表你的行為不合規矩、不合適。好了，你們已輕易取勝了！但在文章完結前，讓我再挑戰一下你的體育精神吧！接下來的幾天，嘗試用盡可能多的體育片語，給你的朋友、同事留下深刻的印象。如果你有留心我的專欄，加以練習之下，相信不難取勝呢。讓我驕傲一番吧，下周再談！\n",
      "\n",
      "Predicted \"zh\" but should be \"en\".\n",
      "Article: \n",
      "Sharethis:(Eng.summarybelow)《開罐Opener》近日訪問了約三十名小學生，（https://www.facebook.com/OpenerHK/videos/10216292218007677/），發覺其學校不單普教中，甚至用普通話教其他科目，例如數學。這些兒童於是覺得香港粵語並非學習知識的語言（「普通話用來教學，廣東話用來鬧人」），日常即使講粵語，都會用普通話的詞彙（如「冷氣」變「空調」；「扮晒嘢」變「裝B」，按：B/逼乃粗口<unk>的諧音）；情況漸似今日的廣州。今日的廣州人，覺得粵語難登大雅之堂，大部份不諳講粵語，即使識講些少粵語，都多用普通話詞彙（如冰箱變雪櫃），而且用普通話句式（如食煞佢講成把它吃光），有普通話口音（如瑞士讀銳士，轟轟烈烈讀兇兇烈烈）。一種語言，一套世界觀（LeraBoroditsky2010）。香港學校繼續以普通話為教學語言，代表香港靈魂的香港粵語好快淪亡。應對方法係大家日日夜夜講香港粵語，用香港粵語創作，大力杯葛普教中，並且質問教育局，所謂兩文三語教育政策，究竟為香港粵語教育做了些甚麼？“Opener’,aHKwebsite(https://www.facebook.com/OpenerHK/videos/10216292218007677/),recentlyinterviewed30primaryschoolstudents,andfoundthatasaresultoftheirschools’adoptingPutonghuaasthemediumofinstruction,theythinkthat“Putonghuaisforteachingandlearning,whileCantoneseisforswearing”,that“whenChinesetextsarereadinPutonghua,itsoundsmoreagreeable.”Intheirdailylife,evenwhentheyspeakCantonese,theywouldusevocabularyofPutonghuaratherthanthatofCantonese,e.g.,forSashimi,theywouldsaysaang1jyu4生魚[saang1jyu4literallymeanslivingfishbutitcanalsomeanblotchedsnakehead]insteadofjyu4-saang1魚生[fishalive];formotorcycle,theysaymo1-tok3-ce1摩托車[transliteration]inplaceofdin6-daan1-ce1電單車[electricbicycle];forshowingoffpompouslyandstupidly,theysayzong1-bi1裝逼[pretendcunt]ratherthanbaan6saai3je5扮晒嘢[tomega-pretend].ThesituationisgraduallyapproachingthatofGuangzhou,China,wheretheCantonesepeople,after70years’educationinPutonghuainsteadoftheirmothertongue,Cantonese,regardCantoneseasalow-classlanguage.MostofthemdonotspeakCantonese.EvenwhentheyspeaksomeCantonese,theyusePutonghuaphrases(e.g.,forrefrigerator,theyusebing1-soeng1冰箱[ice-box],insteadofsyut3-gwai6雪櫃[snowcabinet]),Putonghuasyntax(e.g.,“Iwalk,comparedwithyou,faster”insteadoftheCantonesesyntax,“Iwalkfasterthanyou”),andevenwithPutonghuaaccent(e.g.,“seoi6-si6瑞士Switzerland’hasbecome“jeoi6-si6銳士Switzerland”).Differentlanguagesleadtodifferentworldviews(LeraBoroditsky2010).TheeducatiionpolicyofteachinginPutonghuaisacolonialbrain-washingstrategyonthepartoftheChinesecommuniststodestroyHKCantonese,thesoulofHongKong,themothertongueofmostHongkongers,onethedefactoofficialspokenlanguagesofHKfor170years.IfHongKongschoolscontinuetoteachinPutonghua,HKCantonesewilldeteriorateveryquickly(cf.RobertBauer2002).ThesolutionistospeakHKCantonesenightandday,tocreategreatworksinHKCantonese,tovigorouslyopposetheeducationalpolicyofteachinginPutonghua,andtoquestiontheHKeducationalauthoritieswhathavetheydoneforHKCantoneseeducation,whentheyoftensay,they“aimtoenableHKstudentstobecomebiliterate[writtenChineseandEnglish]andtrilingual[spokenCantonese,EnglishandPutonghua].”普教中係殖民洗腦教育，上述小學生的「老師話課文用普通話讀會好聽啲」、「班主任話識多啲普通話同其他人講嘢方便啲」。這些未來的香港成年人，日常即使講粵語，都會用普通話的詞彙（例如：「電單車」變「摩托車」；行雷講打雷，按：打雷係北方俗語，行雷係古文雅言：《藝文類聚•雨》重雲吐飛電，高棟響行雷；電單車變摩托車；魚生刺身變了生魚片，按；廣東文化中，生魚係<unk>魚的別名）；普通話的句式（如我行路快過你，講成我行路比你快）；情況漸似今日的廣州。今日的廣州人，覺得粵語難登大雅之堂，大部份不識講粵語，即使識講粵語，都係在家中同不識字的長者講，而且用粵語講普通話的詞彙、普通話的句式（如食煞佢講成把它吃光）、甚至有普通話口音（如瑞士讀銳士）。專門研究香港粵語的語言學<unk>授包睿舜（RobertBauer）（二零零二）預測：普通話成為教學語言後一兩代間，香港學生用粵語正式學讀書面漢語之傳統將喪失，粵文書刊出版會萎縮，後生仔女將以粵語為恥，只在家中同老人家講，正如今下的廣州，而九七後大陸人不斷湧入香港，更會化廣東話做本港少數派語言，到本世紀下旬，粵語甚至會絶種。余以為應對方法係大家日日夜夜講香港粵語，寫文白粵交融的香港雅言，創作高水準的香港粵語文學，大戲，港產片，電視電臺節目，翻譯劇，原創劇，香港粵語流行曲，編寫香港粵語辭典（如粵辭正典），粵英字典。香港家長覺醒，反普教中，皆因知曉一旦形成講普通話的環境，仔女將來的飯碗就會被北方殖民搶走，事關你的普通話怎好都好不過大陸人。大中小學要增設香港粵語文化科，承先啟後本土文明。立法會議員要反對資助普教中，爭取香港粵語教學及研究資助，本土電影資助。市民見到教育局的官員，要問其所謂兩文三語教育政策，究竟對香港粵語教育有何貢獻？例如有無教香港粵語拼音法？有無推行香港粵語文化教育？Sharethis:Comments\n",
      "\n",
      "Predicted \"zh\" but should be \"en\".\n",
      "Article: \n",
      "LookattheformulaIdesignedasthispost’stitle,andpleaseagreeorrebukeme.Youknowhowthemediapicturestheideathatwhenyouarealoneinafestivetime,youfeelevenmorelonelythanever,andbeingsinglemeansanextralayeroflonelinessisaddedontopforyou.SoIcapturedaprint-screenofthisHongKongpsychologicalmagazineIwasreadingearlierthisweek(seebelowandsorryIonlyhavetheChineseversion):ittalksaboutaman’scasewherehedevelopeddepressionduetothefactthathewasdumpedbyhisex-girlfriendonthedayofChristmasEve,andthefestiveatmosphereofChristmasmadehimfeelevenworse;heusedtospendhisChristmaswithher.看看我為這篇文章標題設計的公式，你會同意或是反對呢？你知道的，媒體往往會描述當你獨自一個過節日的時候，你會感到比任何時候都要孤獨，而單身意味著加了一層額外的孤獨感。所以我像在本週早些時候看到的一本香港心理學雜誌的文章cap了圖（見cap圖，對不起，我只有中文版）：它談到一個男人的病例，他的抑鬱症誘因是他之前被前女友在聖誕前夕的那天甩了，而聖誕節的節日氣氛使他感覺更糟;他可習慣了和她一起度過聖誕節呀。Next,Iwanttoposeaquestion:ifyouwerehim,wouldyoufinditshamefulifyouhavetoadmitthatbeingdumpedcausesyoutoturnintoamentallyillperson?接下來，我想提出一個問題：如果你是他，如果你要去承認因為前女友拋棄你導致你變成一個精神病人，你會覺得自己好慚愧嗎？Christmastimewouldbethebackgroundinformationwehave.Let’sbreakdownwhatIaskinparts.Firstthingfirst,isitshamefultofeeldepressiveifapersonisaloneinafestiveseason,likeChristmas?Thisoneiseasytoanswer,doesn’tit?Wheneverywhereisfilledwithanatmosphereinwhichpeoplespendtimetogether,whetheraslovers,familygatheringsorfriendshangingouttogether,itisveryreasonabletofeelmorelonelythanusual.Soalthoughbeingsinglemayfeelmorelonely,singlepeoplearen’tlonelyiftheyarespendingtheirChristmastimewithfriendsorfamilymembers.聖誕節將是我們的背景資訊。讓我分解我問的問題。首先，第一件事是，如果一個人獨自一個過節，如聖誕節，感到抑鬱是不是好慚愧呢？這一個部份很容易回答，不是嗎？當到處充滿了人們聚在一起的氣氛，無論是戀人們約會去，家庭聚會或是朋友們一起外出，感覺比平常更孤獨是非常合理的。所以雖然單身可能會感到更孤獨，如果單身人士與朋友或家庭成員度過聖誕節的時間，他們不用感到孤獨的。Next,soisitshamefultohavedevelopeddepressiontriggeredbythepainfulabandonmentbyaex-lover.ThisremindsmeofsomethingthatIwastaughtasakid.IrememberwhenIwaslittle,theadultsinmyfamilytoldmethatyoungpeopleshouldn’tdatetillweenteruniversity,becauseusteenswouldeitherbetooindulgedinthesweetnessofarelationship,orunabletomoveonintimewhenabreakupcomes.接下來，我想問因為被舊情人拋棄的痛苦經歷而引發了抑鬱症又是不是好慚愧呢？這讓我想起我小時候的教導。我記得當我還小的時候，我家裡的大人告訴我，除非進入大學，年輕人不應該談戀愛約會，因為我們青少年會太沉迷於這一種甜蜜關係，或者分手時無法及時抽身。Itdosenotmatteriftheoutcomeoftherelationshiptasteslikehoneyorforcesyoutoshedtears,itisaboutwhetheronecanhandleitwellandtoensurethatlifegoesonsmoothly.Umm,let’ssay,yougetupsetbygettingdumpedjustfewdaysbeforeyourexamsoyouscrewyourexamsup.However,itdosenottrulymakesense,doseit?Atuniversityyoucouldhavetogothroughthesamethingandscrewupyourexams,oratworkyoucouldscrewuptooandyourbosswouldmakeyouredundant.Itisabouthowmuchyourex-lovermeanstoyouastomakethiskindofimpacttoyoureverydaylife.無論關係的結果是甜點的蜂蜜，或是迫使你流眼淚，說到底，這是關於一個人可不可以處理好失戀這回事，確保自己繼續生活順利。嗯，例如說，你因為在考試前幾天被甩而感到不安，所以你的考試「炒粉」了。然而，這是真相的全部嗎？在大學，你可能需要經歷同樣的事情，你的考試表現被影響了，或你都上班了但影響了工作，你老闆會「炒魷」。你前情人對你的意義是甚麼，你的日常生活所產生的影響就自然會是甚麼。Itisindeedmelancholicbutromanticthatsomeoneloveshisexsomuchtotheextentthathegetsdepressionasaresultright?Iappreciatethatsomeonehastreatedarelationshipseriouslyevenheorshegetsdumpedeventually,justlikeweareencouragedtoalwaystryourbestinmanythingsinlife,butdon’tgetmewrong,Iwouldneverencourageanyonetogetdepressionasaconsequenceofapainfulbreakup.有人愛他的前任那麼多，以至於他得到抑鬱症，不是好憂鬱不過浪漫嗎？但就像我們在生活中總被鼓勵要我們嘗試做到最好，我欣賞會認真對待一個感情關係的人，雖然他／她最終被甩了也不緊要。但不要誤會我，我永遠不會鼓勵任何人因為失戀而去患上抑鬱症。BacktothemanIwastalkingaboutthen.Isheguiltyofshametobebroughtdowntotheextentofenteringintoadepressivecondition?Iwouldhavesaidyesbefore.Ihavealwaysbelievedthatanyone,soevenanon-mentallyillpersonwhocriesforabreakupisapersonwhodisappointshimself,lackinghighaimsinhisownlife,andIlookeddownonmyselfsosomucheachtimewhenIeachtimecriedforanyreasons.IhatetoadmitthisbutlikemymyfamilyjokesandsaysthatIamverycapableofcryingsincethedayIwasborn,IhavetoomanyunhappymomentsbecauseIassociatedcryingwithself-devaluation.Onlyrecently,Istarttothinkslightlydifferently:amIstilldisappointingmyselfifIwouldgetmyselftogethereachtimeaftercryingandkeeponachievingthoseaimsofmine?回到那個我在談論的男人。他因為失戀而陷入抑鬱狀態的程度是不是好慚愧呢？我以前會說是的。我一直相信任何人，即使是一個非精神病患者，因為分手而哭泣是一個教自己失望的人，一個在自己的生活中沒了大志的人，所以，每次當我看著自己因為不同的原因哭起來，我都看不起自己。我不願意承認這一點，但像我的家人老是笑話說，我打從出生的那一天就好會哭了，我就有太多的不快樂時候，因為我把哭泣與自我貶值連繫了。只是最近我才開始有點不同：，如果我每次都在哭泣後讓自己在去繼續實現我的目標，我是不是還是教自己失望呢？Isupposeit’sapsychiatristwhohadtohelpthismantogetoverhisbreakup,andIimagineabitlikecuringaPTSD(post-traumaticstressdisorder).Iknownothingaboutpsychiatry,butcommonsensecanpointoutthatthebreakupisjustoneofmanytriggeringfactors,andevenitseemstobelittleaman(orawoman)whenwehearhimgettingdepressionforsuchareason,ratherthanforsomething‘big’likelosingajobordeathorsomeone.Iwouldhavetoreiteratethatitisnotsomeonechoosestobetriggeredbyaspecificfactor.Foreachdepressionpatient,acertainfactorhappenstoappearinhislifetoleadhimtosufferfromdepression.It’seasiertounderstandifwethinkabouthowweacceptthatquiteoftenthecauseofsomeonesufferingfromcancerisalsounknown.IthinkIunderstandwhyjobslikemarriagecounsellorsbesidespsychiatristsandtherapistsexistinthepsychologicalfield.我想是透過一個精神病醫生的治療，那個男人就走出失戀的陰霾吧，我想像成像治愈PTSD（創傷後遺症）一樣。我對精神病學一無所知，但是常理可以指出，失戀只是許多觸發抑鬱症的因素之一，雖說當我們聽到一個男人（或一個女人）因為這樣的原因而感到抑鬱時，似乎他們被貶低了，因為可能我們會想，應當是一些比較「高價值」的原因，像失去工作，有人死亡才會讓人患上抑鬱症吧。我必須重申，不是人可選擇某個特定因素去觸發自己變成患者。對於每個抑鬱症患者，一定的因素恰好出現在他的生活中，導致他患有抑鬱症。如果我們想想我們如何經常接受癌症患者患癌的原因未能解釋，這就會更容易理解抑鬱症病人。我想我明白為什麼除了精神病醫生和治療師，婚姻輔導員這樣的工作存在於心理領域。Youwillrealiseafterall,Iamwritingtotalkaboutmyself.IamtalkingabouthowmuchIusedtolookdownonmyselfbecauseIeasilyburstedintotears,Ifeltsodepressivewhenmyex-loverleftme,andIwouldfeelshamefulbecauseIstronglybelievedthatbycryingalotduetomanyothersreasonstogetherwiththebreakup,Iamnothingbutacoward.Iamworkingonhowtostopcallingmyselfacoward,apieceofrubbish,thingslikethat.你會意識到，最後，我在談論我自己。我在說我有多麼看不起自己，因為我很容易哭，還有前情人離開我時我感到多麼抑鬱，我感到好慚愧，因為我堅信，由於失戀或許多其他原因而哭，我就是一個懦弱的人。我正在努力如何停止給自己與懦弱，一塊垃圾這些東西掛上等號。SonowIhavewrittenupthispostontheChristmasEve,andIjustwannasay,singleornot,lonelyornot,myformulaisacompletebullsh*tbecauseyoucancertainlyfindyourjoyinyourownmeans.Iamtryingtocreatejoyformyself.ItellyounowwhatIamgoingtodoonthe25th:Iamvolunteeringagain,maybedoabitof‘peoplewatching’inacafétoreadafterwards(myexamsarecomingup…)andbytheendoftheday,justgohomeforasimpledinner.現在是平安夜的時間，我只是想說，單身又好，獨自一個又好，我那公式一定是bullsh*t，因為你可以用你的方法找你的快樂。我在為自己制造快樂。我現在告訴你我25號會做甚麼好了：我又去做義工啦，可能之後去找個café去看看人來人往的景象再看看書（我快考試了。。。。。。），而再晚些就回家去吃一頓普通的晚飯吧。MerryX’mas.聖誕快樂。P.S.Atinypieceofinterestinginformation,seethepictureandpleasedon’tmixupmistletoeandhollyeveragain.有個有趣的小資訊，看看圖，其實不是聖誕花在英文有兩種說法的啦，但中文，「聖誕花」就是「聖誕花」吧！\n",
      "\n",
      "Predicted \"hky\" but should be \"en\".\n",
      "Article: \n",
      "JasonChow's SpeechonHongKongNationalParty-“DefendforDemocracy,HongKongIndependence”RallyTranscribedbyWilliam,translatedbyWilliamandSidney,spokenbyJasonChowHo-fai[TheEnglishtranslationisreleasedunderCreativeCommons,CCBY-NC-ND2.0](Source:Undergrad,HKUSUInstantNews)Helloeveryone,IamthespokespersonoftheHongKongNationalParty.Well,ontherallylastFriday,ImistakenlysaidthatIwastheconvener-actually,Iamthespokesperson,alright.MynameisChowHo-fai,sotodaywehavesuchalargeandpeacefulgathering;manyweredispleasedwithpeacefulgatherings,orperhapsdoubtful,andthatisbecausethePan-Democraticcamphasforthelastnineteenyears,squanderedeachandeverychancewhenwehadone.Peacefulassembliesweresupposedtobeawaytogatherthecrowd,suchthatwecanwaitforanopeningtoputupafightonthestreetsortolaunchanoperationagainstthegovernment.YetthePan-Democratsdismissedthecrowdandsendeveryonehomeattheboilingpoint,timeaftertime.Manyagreatopportunityneverhavethechancetobecomesomething.BeforeIsaywhatIamsupposedtosaytoday,letmetalkaboutourpublicitymaterials.Itrustthateverybodyheremusthavereceivedleafletsonthefootbridgeoverthere,whenyoucameherefromtheAdmiraltyCentre.Ononeside,itjustsays“Independence!”followedbyanexclamationmark.Sowhatisitallabout?Ifyouareholdingone,youcantakealookrightnow.Actually,itliststhedoubtsthatmanyHongKongpeople,manynormalcitizenshave:howwoulditbepossibleforHongKongtobecomeindependent,orspecifically,whydoesHongKonghavewhatittakestobecomeanindependentsovereignstate.Theyalwayssay,“ifwedeclareindependence,thenyouwon’thaveawatersupply,andyouloserswouldstarve”.Thatisnottrueactually,HongKongdoesnotrelyonChinamuchintermsoffoodsupply.Infact,Chinaisthebiggestfoodimporterintheworld.Asforfreshwater,SingaporeutilizesdesalinationandHongKongcanfollowtheirexample.ManysaidthattheyaregoingtoletloosethePLA,butthentheydidnotdothatonthe28thofSeptember,theyjustcalledthepolice.Whydidtheynotdothatisimportant-HongKongisaplacethatholdsalotofcapitalandpropertyforXiJinping,hisfamiliesandmanymoreCCPhigher-ups,astheyshifttheirassetsoutofChina.Assuch,suggestingthatthePLAwouldmarchoutandpurgeHongKongispreposterousandIdonotbelievethatitcouldhappen.ThatPLAbarracksoverthere,don’thavethewrongideathatithousesawholebunchoftough,seasonedwarriors.TheonesaregarrisonedinHongKong,aretheoneswhogotherevia“guanxi”(specialrelationships).BecauselifeintheHongKongGarrisoniswonderful,itisnottaxingatall.ShallweHongkongersbeafraidofthem?Ofcoursenot.Whyhaven’tHongkongersmakeabreakthroughpoliticallyalltheseyears?It’sbecauseourgreatestfearlivesinusourselves.Hongkongerslackthecouragetofacetheirfears.Theythinkthatwhoevermakesthefirstmovewillbesuppressed,orevenhavetheirpropertyraidedandconfiscated.ThisfearcamefromtheJune4thincident,whentheoldergenerationswitnessed,eitherontelevisionorattheTiananmenSquare,thePLAslaughteringthepeoplewithtanksandarmies.Then,ofcourseIshouldbetalkingabouttheconditionsthatmakeHongKongindependencepossible.IwillnowstartwhatIreallywanttotalkabouttoday.Hongkongershavefacedmanydifficultiesandhardshipsinthesenineteenyears.EversincetheHandoverin1997,Hongkongerswentfromfightingforhighidealssuchaspoliticalrights,tostrugglingforbasicsubsistencelikelivingspace.Whatwearefacingtoday,whatmanyHongkongersarefacingtodayisnotjustthelossofpoliticalrightsorfreedom,butanexistentialthreat.YouHongkongerswouldfinditextremelydifficulttorentaflat,andevenanicheforyourasheswouldbeahardtocomeby.It’shardenoughtosortoutthefuneralritesinHongKongafteryoudie,nottomentionactuallylivinghere.Wehavegonefromthepursuitofdemocracyandliberty,tostrugglingforbasicsurvivalrightsandspace.ThenwhatmadeHongkongersliveinagonyforthelastnineteenyears?Itwasthatfatefulnightin1997,whenHongkongersandHongKongdidnotchoosearoadtoindependenceandself-sufficiency,buttheychose,orrather,thoseso-calledintellectualsandeliteschoseforallHongkongers,andalsothenextgeneration,toreturntoChina.Theythinkthat“democraticreturntoChina”istherightthingtodo,theythinkitistheonlyproperandjustcourse.AndsoHongkongers,includingourgeneration,andthefuturegenerations,havetobeartheconsequences.Wehavetostruggleforourownmostbasicrighttoliveandbasicfreedoms.ActuallywhatIamtryingtosayisHongkongersabsolutelyhavetheabilitytogovernourselves.IbelieveHongkongersdopossesstheabilitytocreateahappysociety,andIbelieveHongkongershavetheabilitystandamongsttheWestorJapan,amongstthesecountriesandnotbefoundwanting.YetwhydoHongkongerstodayhavetoworryaboutwhetherournominatedcandidateswouldbedisqualified,andfeelgraciouswhentheyaren’t?Whyhavewesunksolow?ManypeopleclaimthatthereissomethingwrongwithHongKong,butno,HongKongisfine.AlltheproblemsarewiththeChinese,Chinaisthecruxoftheproblem.TheChinesegovernmentistheproblem,notyouHongkongers.Hongkongershavewastedtoomuchtime.Youhavedonenothingatallinthesenineteenyears.Bynothing,Imeanthatallyouhavedoneisparticipatinginpeacefulgatherings-thereisnothingwronginherentlywiththegatherings,butyougainednothingvaluablefromthem.YoudidnotstarttoorganizetheresistancebackthenwhenHongKongfelltoChina.That’swhatwentwrong.Wehavestartedlate,weneedtocatchup.ManyquestionedhowcouldwepossiblyrealizeHongKongIndependence,ortheysayordinarycitizenswouldnottakeheedbecausetheyvaluetheirjobmorethantheirrights.Myresponseisthis:everyHongkongerhavetheirownparttoplayinfurtheringthegoalofHongKongIndependence.Everyonecandosomethingwithintheirownabilitiesandpositions,likeHitsujikojustsaid-ifyouarepartofthemiddle-class,youcanmakemonetarycontributionsorsupportthemovement;ifyouareworkinginIT,youcanhelpIndependentistorganisationsbuildtheirwebsites,can’tyou?IamconfidentthateverycitizenhavearoletoplayinpromotingHongKongIndependence.Theyeachhavetheabilitytocontribute.Manypeople(mistakenly)claim:“oh,revolutionmustbeasuddenoutburst,itmustbedonewithguns,theremustbeconfrontationwiththepolice,orthrowingbricks”,sooneandsoforth.Butrevolutionnotonlyasuddenoutburstofpassion,itisalsoarationalandcalculatedmove.SunYat-senattemptedagrandtotalofelevenrevolutionsinChina.Theeleventhtimefinallyworkedout,andthat,theRevolutionof1911wasjustafluke-theHupehgarrisonwasrotatedintoSzechwan.Still,withoutDrSun,orHuangHsingandtheReviveChinaSociety,withoutthemevangelizingthecauseintheSouthSeas,withoutthemspreadingideasaboutrevolution,therevolutionwouldhaveneveroccurredinthefirstplace.AndnowwhatHongkongersshoulddointheselongandbroodingdays,besideswaitingfortheoncomingrevolution,istospreadthewords.Talktoyourcolleagues,yourparents,yourfriends-tellthemwhyHongKongmustbecomeindependentnow,whyHongKongIndependenceistheonlywayoutforeachandeverycitizen.Thepathtoindependencemaybeperilous,butlooktothosestandingwithyou,lookatthosefamiliarfaces.Instrivingtowardsindependence,youwillnotbealone.InthesenineteenyearsthePan-Democratshavepreachedtheconceptofdemocracyatrallies,buthowmanytrulyunderstandwhatdemocracyis?ThePan-Democratshavetheirinterpretation,thePro-Establishmentshaveanothertwistedandtorturedinterpretation,yetIreckonthattherearetwoconceptscentraltodemocracy:first,thepeople,thatisthenationofHongKong;secondly,thesovereigntyofHongKong.ThePan-DemocratsalwayssaythatevenunderthedominionofChina,wecanstillfightfordemocracy,butwecan’t.TheElectoralAffairsCommissiondeprivedsomanypro-independencecandidatesoftheirrighttoruninanelection,thispreciselyshowsthatunderthegripofChina,youHongkongerswouldabsolutelynotbeabletocalltheshots.Democracyisjustself-governance,self-determination.YetundertheoppressionofChina,itcanneverbeachievedbyHongkongers.Whywehavetodeclareindependence,isbecausewehavetotakebackoursovereignty,beforewecancontrolourdestiny.Manypeoplesaythat“peaceful,rational,non-violent,non-profane”assembliesareuseless,butIdisagree,becausethisisthefirsttimetheideaofHongKongIndependenceisopenlypromotedtoallHongkongersinsuchapublicmannerandinsuchalargescale.Todayisjustaforum,arallyatthisTamarPark,avenuewherewecanexchangeideas.ButIhopethatinthefuture,intheforeseeablefuture,wecanreturntothisplace,notforanotherrally,butforeverycitizentogatheraroundandmakeourownHongKong’sTennisCourtOath!Thankyoueveryone.大家好，我係香港民族黨嘅發言人！咁我上次禮拜五集會講錯咗召集人，咁今次講返啱啦，我係發言人，係嘞。咁我叫周浩輝，咁今日就有一個咁大型嘅和平集會啦，好多人對和平集會都有一個厭惡，或者係一個質疑啦，咁因為泛民主派過去十九年都係玩爛咗呢一個和平集會。其實suppose和平集會係for積聚呢一個人流啦，然後就等一個爆發點，然後就係有一啲街頭抗爭，或者係一個反政府嘅行動，咁但係泛民主派過往就係喺民怨最沸騰或者最激昂嘅時候呢，就同大家講「請沿著呢一個維園啦，譬如維園嘅話嘞，請沿著邊一度嘅出口就離開，跟住返屋企」咁樣樣。咁就令到好多好好嘅機會呢，都無機會爆發咁樣樣。咁呀今日喺正式我想講啲嘢之前，我就講下我哋民族黨嘅一啲宣傳品嘅，咁我相信大家嚟嘅時候呢，喺天橋呢，姐金鐘嗰邊姐海富天橋啦，都會收到份文宣嘅，咁上面呢寫住獨立！跟住一個感嘆號。咁究竟呢份嘢講咩呢，咁你有揸喺手你可以宜家舉高啦，姐係去睇下啦，其實入面就係寫住好多香港人或者係好多一般嘅市民都質疑嘅一樣嘢，究竟香港可以點樣獨立，或者，點解香港係有能力成為一個獨立嘅主權國家。佢哋好鍾意講嘅，「喂宜家獨立咁香港冇水飲喎，餓死你班廢青啦」，咁其實唔係嘅，咁，香港其實有好多，姐糧食方面係唔係依賴中國嘅，中國其實係第一大嘅糧食進口國嚟，水方面其實新加坡係用緊海水化淡。咁香港亦都可以仿效。咁軍隊方面好多人就話會出解放軍屠城，咁，咁九二八都無出解放軍啦，淨係出咗啲警察姐，咁點解佢唔出解放軍係好重要，因為香港呢一撻地方係收埋咗習近平及其親屬同埋呢一個中共高層好多資金、資產喺香港，作為一個資產轉移之用啦，咁樣。咁所以，話解放軍係會出動然後係鎮壓香港，呢個係一個匪夷所思啦，我亦都唔相信呢樣嘢會發生。咁軍隊方面好多人就話會出解放軍屠城，咁，咁九二八都無出解放軍啦，淨係出咗啲警察姐，咁點解佢唔出解放軍係好重要，因為香港呢一撻地方係收埋咗習近平及其親屬同埋呢一個中共高層好多資金、資產喺香港，作為一個資產轉移之用啦，咁樣。咁所以，話解放軍係會出動然後係鎮壓香港，呢個係一個匪夷所思啦，我亦都唔相信呢樣嘢會發生。其實姐隔離嘅解放軍軍營呢，大家唔好以為住著嗰啲，姐皮膚黝黑呀，好健碩嘅解放軍，其實嚟得香港做駐軍嘅呢，全部都係靠關係疏通嘅，因為喺香港做駐軍呢，就好正嘅，呀，唔使咁辛苦嘅。咁所以解放軍係咪一個香港人應該恐懼嘅對象，或者係香港人係要突破嘅一個心魔，其實唔係。香港點解咁多年都唔能夠係喺呢個叫政治上做突破，係因為香港人其實面對最大嘅心魔就係自己。因為香港人無勇氣係去面對著心裡面嗰份恐懼，覺得凸個頭出嚟就會俾人鎮壓，或者係會俾人抄家，呢一份恐懼係嚟自一九八九年六四，老一輩親眼喺電視機又好，或者喺北京嘅現場都好，目睹嘅坦克屠城、解放軍屠城，所導致嘅，咁樣樣。咁呀，當然講返啲香港能夠獨立嘅條件啦，正式開始我想講嘅嘢。其實香港人好多苦難嘅，十九年間，啊。一九九七年主權移交之後香港人，由開初話想爭取呢一個政治權利，去到今日，大家為著你哋自己最基本、最基本嘅生存空間，喺度掙扎奮鬥。我哋今日面對嘅，好多香港人面對嘅唔單單係無咗政治權利，或者無最基本嘅自由，而係連你哋最基本嘅生存都受到威脅。你哋香港人非常之難，搵一個租所去住啦，甚至係你哋死左之後呢，你哋嘅龕位呢，都係一龕難求嘅。姐係香港人係連死呀，都係十分之困難嘅。可想而知你話喺香港生存係更加困難嘅一樣嘢。咁究竟係啲乜嘢係令到香港人，喺呢十九年間痛不欲生，由初初只不過係追求呢一個民主自由，去到今日要為自己最基本嘅生存權利同埋空間係去奮鬥，就係因為一九九七年，嘅嗰一個夜晚，香港人、香港唔係選擇咗一個獨立自強嘅道路，而係選擇咗，或者，唔係你哋選擇啦，係有一班所謂嘅知識分子啦，所謂嘅精英啦，係去越俎代庖，幫你哋咁多香港人選擇咗，幫你哋香港人嘅下一代選擇咗，要所謂回歸中國，佢哋覺得民主回歸，係一個正確嘅事，係民主回歸先至係正確，係大義所在，結果今日嘅香港人，包括我哋呢一代，同埋呢一代以後啦，都要承受呢一種嘅惡果，都要為著自己最基本嘅生存嘅權利同埋自由，係去掙扎。其實我想講就係，香港人其實係絕對有能力，係去自己管治自己嘅。我相信香港人係有咁樣嘅能力，係去創造一個幸福嘅社會，我亦都相信香港人，係有咁樣嘅能力喺國際之間，係能夠出埠，係可以同歐美先進國家，可以同日本呢啲咁好嘅國家，係鼎足而立。但點解今日嘅香港人要淪落到一個地步，就係話一場立法會嘅選舉，「啊，好擔心入唔入到閘，入到閘，我就要慶祝一番」，會淪落到咁呢？好多人話香港嘅問題，其實香港嘅問題，香港一路都冇問題，所有嘅問題都係中國人嘅問題，因為中國先至係問題嘅所在，中國政府先係問題嘅所在，唔係香港人你哋。香港人虛耗嘅時間其實係太多喇。因為呢十九年間，你哋，係無做過嘢嘅。冇做過嘢嘅意思就係，你哋十九年間都係做著一啲和理非非嘅一個嘅集會，而和理非非嘅集會唔係有問題，個本質係冇問題，但你哋從和理非非嘅集會，從泛民組織嘅集會嗰度，係得唔到任何嘢，你哋亦都冇喺香港淪陷嘅嗰一刻，開始組織反抗嘅力量，呢個先係問題嘅所在。香港人你哋起步得太遲喇，你哋要急起直追。好多人話「嘩，你搞呢個香港獨立，搞港獨，你點樣搞啊」或者話呀，香港人，姐我哋講㗎，「港豬點會理你哋呀，佢哋仲要返工」，我想講嘅係，其實每一個香港人，係推動香港獨立，呢個目標上面，都可以有自己角色，都可以有自己能夠做到嘅嘢，好似呀羊子啱啱咁講，如果你係中產嘅，你咪盡你最基本嘅能力，做呢個叫金主或者後援嘅角色囉；如果你係搞IT嘅，呀，咁你咪去幫呢一個叫做獨派嘅組織，係去做website囉，係咪？我相信香港社會，任何一個人，喺推動香港獨立呢一個議程上面，都有佢嘅角色，都有佢嘅能力，都可以有佢嘅貢獻。好多人話，「啊，革命一定係要嗰當下就要爆發，一定係要揸著槍，然後就係同呢個警察衝突呀，或者係掟磚」咁樣。革命，係一場一瞬間嘅情感爆發，更加係一場理性，同埋計算嘅一個鋪排。孫中山佢搞中國嘅革命，佢搞咗足足十一次，第十一次先成功，而第十一次成功嗰一次，辛亥革命，佢只不過係符碌，湖北嘅駐軍調咗去四川。但係，如果無孫中山，或者黃興一班咁樣嘅興中會，係去不停喺南洋播道呢一個中國革命，或者係做一個思想傳播嘅工作，革命係唔會發生。而香港人，宜家要做嘅，除咗係要慢慢等待嗰場嘅革命嘅爆發之外，更加要係革命爆發呢一段長久、鬱悶、屈躁難耐嘅一段等待嘅時間去做嘢，做嘅嘢就係思想嘅傳播。係去同你嘅同學講，同你嘅父母講，同你嘅朋友講，點解香港去到呢一刻一定要獨立，點解香港獨立先至係所有香港人嘅出路。香港獨立呢條路可能好艱難，但望下你身邊嘅香港人，望下你身邊熟悉嘅面孔，喺香港獨立呢一個目標或者進程上面，你會有你嘅支持。十九年間，泛民主派喺和平集會向香港人講好多民主嘅理念，但有幾多人知道民主究竟係啲咩？泛民主派對於民主有一套演繹，建制派對民主又有第二種嘅扭曲，但我認為嘅民主，係有兩個概念，第一：民，係指香港民族；第二：主，係指香港嘅主權。泛民主派經常話喺呢個中國嘅殖民統治之下，我哋可以爭取到民主，但我哋唔可以。因為今次選管會剝奪咁多位主張香港獨立候選人嘅參選權利，就正正證明咗喺香港，喺中國嘅殖民統治之下，你哋香港人係絕對唔能夠自己話事。民主就係自己管理自己，自己決定自己嘅命運，但喺中國嘅殖民統治之下，呢一點係香港人永遠都無可能做得到。點解要獨立，就係因為香港人，要掌握自己嘅主權，先至可以掌握到自己嘅命運。今日呢度好多人話和理非非無用，但我唔認同，因為香港獨立呢個理念，係第一次咁公開、咁大型，向全香港人傳播。香港人喺未來嘅路上，我好希望，姐今日呢，就只不過係添馬公園呢度，係做一個演講啦，或者係一個集會啦，大家可以交流啦，我好希望就係，喺不久嘅將來，喺可見嘅將來，大家可以一齊重來呢個地方，並唔係再為咗再一次舉行呢個集會，而係到時我哋全香港嘅人可以聚集喺呢度，發表我哋香港自己嘅網球場宣言！多謝各位。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        predictions = selected_model(batch.text)\n",
    "        max_preds = predictions.argmax(dim=1)\n",
    "        wrong_preds = (max_preds.eq(batch.label) == 0).nonzero()\n",
    "        for wrong_pred in wrong_preds:\n",
    "            wrong_idx = wrong_pred.item()\n",
    "            incorrect_prediction = max_preds[wrong_idx].item()\n",
    "            correct_label = batch.label[wrong_idx].item()\n",
    "            print(\"Predicted \\\"\" + LABEL.vocab.itos[incorrect_prediction] + \"\\\" but should be \\\"\" + LABEL.vocab.itos[correct_label] + \"\\\".\")\n",
    "            text_i = batch.text[:,wrong_idx].tolist()\n",
    "            text_striped_idx = [x for x in text_i if x != TEXT.vocab.stoi['<pad>']]\n",
    "            full_text = list(map(lambda x: TEXT.vocab.itos[x], text_striped_idx))\n",
    "            print(\"Article: \")\n",
    "            print(\"\".join(full_text))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
